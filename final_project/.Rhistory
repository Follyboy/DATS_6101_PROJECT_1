fill = c("green","blue", "red"))
plot(ROC_rf, col = "green", main = "ROC For all the three models")
plot(h, add=TRUE, col="red")
plot(h2logistic, add=TRUE, col='blue')
legend(0,0.2,legend=c("Random Forest", "Logistic Regression", "KNN"),
fill = c("green","blue", "red"))
plot(ROC_rf, col = "green", main = "ROC For all the three models")
plot(h, add=TRUE, col="red")
plot(h2logistic, add=TRUE, col='blue')
legend(0,0.4,legend=c("Random Forest", "Logistic Regression", "KNN"),
fill = c("green","blue", "red"))
plot(ROC_rf, col = "green", main = "ROC For all the three models")
plot(h, add=TRUE, col="red")
plot(h2logistic, add=TRUE, col='blue')
legend(0.1,0.4,legend=c("Random Forest", "Logistic Regression", "KNN"),
fill = c("green","blue", "red"))
plot(ROC_rf, col = "green", main = "ROC For all the three models")
plot(h, add=TRUE, col="red")
plot(h2logistic, add=TRUE, col='blue')
legend(0.3,0.4,legend=c("Random Forest", "Logistic Regression", "KNN"),
fill = c("green","blue", "red"))
#before smote training
logitModel1_b <- glm(stroke~ ., data = beforeSMOTE, family = binomial())
logitModel2_b <- glm(stroke~ age + age:hypertension + age:heart_disease +age:avg_glucose_level, data = beforeSMOTE, family = binomial())
logitModel3_b <- glm(stroke~  age*heart_disease +age*glucoseGroup, data = beforeSMOTE, family = binomial())
logitModel4_b <- glm(stroke~ (age+heart_disease+ avg_glucose_level)^2 , data = beforeSMOTE, family = binomial())
logitModel5_b <- glm(stroke~ (age + glucoseGroup)^2 , data = beforeSMOTE, family = binomial())
logitModel6_b <- glm(stroke~ age+age:heart_disease+ age:avg_glucose_level , data = beforeSMOTE, family = binomial())
#selecting model for further analysis of code
logitModel_b <-logitModel3_b
summary(logitModel_b)
pR2(logitModel_b)
library(caret)
library(dplyr)
library("pROC")
library(randomForest)
library(lessR)
#install.packages("gridExtra")
library("gridExtra")
library(tidyr)
# Useful functions when working with logistic regression
library(ROCR)
library(grid)
library(caret)
library(scales)
library(ggplot2)
library(gridExtra)
library(data.table)
library(caret)
library(pscl)
library(DMwR)
#before smote training
logitModel1_b <- glm(stroke~ ., data = beforeSMOTE, family = binomial())
logitModel2_b <- glm(stroke~ age + age:hypertension + age:heart_disease +age:avg_glucose_level, data = beforeSMOTE, family = binomial())
logitModel3_b <- glm(stroke~  age*heart_disease +age*glucoseGroup, data = beforeSMOTE, family = binomial())
logitModel4_b <- glm(stroke~ (age+heart_disease+ avg_glucose_level)^2 , data = beforeSMOTE, family = binomial())
logitModel5_b <- glm(stroke~ (age + glucoseGroup)^2 , data = beforeSMOTE, family = binomial())
logitModel6_b <- glm(stroke~ age+age:heart_disease+ age:avg_glucose_level , data = beforeSMOTE, family = binomial())
#selecting model for further analysis of code
logitModel_b <-logitModel3_b
summary(logitModel_b)
pR2(logitModel_b)
AIC(logitModel_b)
#after smote training
logitModel1_a <- glm(stroke~ ., data = afterSMOTE, family = binomial())
logitModel2_a <- glm(stroke~ age + age:hypertension + age:heart_disease +age:avg_glucose_level, data = afterSMOTE, family = binomial())
logitModel3_a <- glm(stroke~ age*heart_disease +age*glucoseGroup, data = afterSMOTE, family = binomial())
logitModel4_a <- glm(stroke~ (age+heart_disease+ avg_glucose_level)^2, data = afterSMOTE, family = binomial())
logitModel5_a <- glm(stroke~ (age + glucoseGroup)^2, data = afterSMOTE, family = binomial())
logitModel6_a <- glm(stroke~ age+age:heart_disease+ age:avg_glucose_level , data = afterSMOTE, family = binomial())
#selecting model for further analysis of code
logitModel_a <- logitModel3_a
summary(logitModel_a)
pR2(logitModel_a)
AIC(logitModel_a)
beforeSMOTE$prediction <- predict( logitModel_b, newdata = beforeSMOTE, type = "response" )
test$prediction  <- predict( logitModel_b, newdata = test , type = "response" )
par(mfrow = c(1, 2))
ggp1 <- ggplot( beforeSMOTE, aes( prediction, color = as.factor(stroke) ) ) +
geom_density( size = 1 ) +
ggtitle( "Training Set's Predicted Score before SMOTE" )
ggp2 <- ggplot( test, aes( prediction, color = as.factor(stroke) ) ) +
geom_density( size = 1 ) +
ggtitle( "Testing Set's Predicted Score before SMOTE" )
grid.arrange(ggp1, ggp2, ncol = 2)
afterSMOTE$prediction <- predict( logitModel_a, newdata = afterSMOTE, type = "response" )
testAfterSMOTE$prediction  <- predict( logitModel_a, newdata = test , type = "response" )
testAfterSMOTE <- test
afterSMOTE$prediction <- predict( logitModel_a, newdata = afterSMOTE, type = "response" )
testAfterSMOTE$prediction  <- predict( logitModel_a, newdata = test , type = "response" )
par(mfrow = c(1, 2))
ggp1 <- ggplot( afterSMOTE, aes( prediction, color = as.factor(stroke) ) ) +
geom_density( size = 1 ) +
ggtitle( "Training Set's Predicted Score after SMOTE" )
ggp2 <- ggplot( testAfterSMOTE, aes( prediction, color = as.factor(stroke) ) ) +
geom_density( size = 1 ) +
ggtitle( "Testing Set's Predicted Score after SMOTE" )
grid.arrange(ggp1, ggp2, ncol = 2)
ConfusionMatrixInfo <- function( data, predict, actual, cutoff )
{
predict <- data[[predict]]
actual  <- relevel( as.factor( data[[actual]] ), "1" )
result <- data.table( actual = actual, predict = predict )
result[ , type := ifelse( predict >= cutoff & actual == 1, "TP",
ifelse( predict >= cutoff & actual == 0, "FP",
ifelse( predict <  cutoff & actual == 1, "FN", "TN" ) ) ) %>% as.factor() ]
# jittering : can spread the points along the x axis
plot <- ggplot( result, aes( actual, predict, color = type ) ) +
geom_violin( fill = "white", color = NA ) +
geom_jitter( shape = 1 ) +
geom_hline( yintercept = cutoff, color = "blue", alpha = 0.6 ) +
scale_y_continuous( limits = c( 0, 1 ) ) +
scale_color_discrete( breaks = c( "TP", "FN", "FP", "TN" ) ) + # ordering of the legend
guides( col = guide_legend( nrow = 2 ) ) + # adjust the legend to have two rows
ggtitle( sprintf( "ConfusionMatrix Cutoff : %.2f", cutoff ) )
return( list( data = result, plot = plot ))
}
# visualize .6 cutoff (lowest point of the previous plot)
# cm_info <- ConfusionMatrixInfo( data = test, predict = "prediction",
#                                 actual = "stroke", cutoff = .08 )
# #ggthemr("flat")
# cm_info$plot
par(mfrow = c(1, 2))
list1 <- ConfusionMatrixInfo( data = test, predict = "prediction", actual = "stroke", cutoff = .02 )
list2 <- ConfusionMatrixInfo( data = testAfterSMOTE, predict = "prediction", actual = "stroke", cutoff = .27 )
ggp1 <- list1$plot
ggp2 <- list2$plot
grid.arrange(ggp1, ggp2, ncol = 2)
# ------------------------------------------------------------------------------------------
# [AccuracyCutoffInfo] :
# Obtain the accuracy on the trainining and testing dataset.
# for cutoff value ranging from .4 to .8 ( with a .05 increase )
# @train   : your data.table or data.frame type training data ( assumes you have the predicted score in it ).
# @test    : your data.table or data.frame type testing data
# @predict : prediction's column name (assumes the same for training and testing set)
# @actual  : actual results' column name
# returns  : 1. data : a data.table with three columns.
#            		   each row indicates the cutoff value and the accuracy for the
#            		   train and test set respectively.
# 			 2. plot : plot that visualizes the data.table
AccuracyCutoffInfo <- function( train, test, predict, actual )
{
cutoff <- seq( .01, .9, by = .05 )
accuracy <- lapply( cutoff, function(c)
{
data_train <- as.factor( as.numeric( train[[predict]] > c ) )
cm_train <- confusionMatrix(data_train, as.factor(train[[actual]]),positive="1" )
data_test <- as.factor( as.numeric( test[[predict]] > c ) )
cm_test  <- confusionMatrix( data_test, as.factor(test[[actual]]),positive="1" )
dt <- data.table( cutoff = c,
train  = cm_train$overall[["Accuracy"]],
test   = cm_test$overall[["Accuracy"]] )
return(dt)
}) %>% rbindlist()
# visualize the accuracy of the train and test set for different cutoff value
# accuracy in percentage.
accuracy_long <- gather( accuracy, "data", "accuracy", -1 )
plot <- ggplot( accuracy_long, aes( cutoff, accuracy, group = data, color = data ) ) +
geom_line( size = 1 ) + geom_point( size = 3 ) +
scale_y_continuous( label = percent ) +
ggtitle( "Train/Test Accuracy vs Cutoff" )
return( list( data = accuracy, plot = plot ) )
}
par(mfrow = c(1, 2))
list1 <- AccuracyCutoffInfo( train = beforeSMOTE, test = test, predict = "prediction", actual = "stroke" )
list2<- AccuracyCutoffInfo( train = afterSMOTE, test = testAfterSMOTE, predict = "prediction", actual = "stroke" )
ggp1 <- list1$plot
ggp2 <- list2$plot
grid.arrange(ggp1, ggp2, ncol = 2)
SensitivityCutoffInfo <- function( train, test, predict, actual )
{
cutoff <- seq( .01, .9, by = .05 )
accuracy <- lapply( cutoff, function(c)
{
data_train <- as.factor( as.numeric( train[[predict]] > c ) )
cm_train <- confusionMatrix(data_train, as.factor(train[[actual]]),positive="1" )
data_test <- as.factor( as.numeric( test[[predict]] > c ) )
cm_test  <- confusionMatrix( data_test, as.factor(test[[actual]]),positive="1" )
dt <- data.table( cutoff = c,
train  = cm_train$byClass[["Sensitivity"]],
test   = cm_test$byClass[["Sensitivity"]] )
return(dt)
}) %>% rbindlist()
accuracy_long <- gather( accuracy, "data", "accuracy", -1 )
plot <- ggplot( accuracy_long, aes( cutoff, accuracy, group = data, color = data ) ) +
geom_line( size = 1 ) + geom_point( size = 3 ) +
scale_y_continuous( label = percent ) +
ggtitle( "Train/Test Sensivity vs Cutoff" )+
ylab("Sensitivity")
return( list( data = accuracy, plot = plot ) )
}
par(mfrow = c(1, 2))
list1 <- SensitivityCutoffInfo( train = beforeSMOTE, test = test, predict = "prediction", actual = "stroke" )
list2<- SensitivityCutoffInfo( train = afterSMOTE, test = testAfterSMOTE, predict = "prediction", actual = "stroke" )
ggp1 <- list1$plot
ggp2 <- list2$plot
grid.arrange(ggp1, ggp2, ncol = 2)
PrecisionCutoffInfo <- function( train, test, predict, actual )
{
cutoff <- seq( .01, .9, by = .05 )
accuracy <- lapply( cutoff, function(c)
{
data_train <- as.factor( as.numeric( train[[predict]] > c ) )
cm_train <- confusionMatrix(data_train, as.factor(train[[actual]]),positive="1" )
data_test <- as.factor( as.numeric( test[[predict]] > c ) )
cm_test  <- confusionMatrix( data_test, as.factor(test[[actual]]),positive="1" )
dt <- data.table( cutoff = c,
train  = cm_train$byClass[["Precision"]],
test   = cm_test$byClass[["Precision"]] )
return(dt)
}) %>% rbindlist()
accuracy_long <- gather( accuracy, "data", "accuracy", -1 )
plot <- ggplot( accuracy_long, aes( cutoff, accuracy, group = data, color = data ) ) +
geom_line( size = 1 ) + geom_point( size = 3 ) +
scale_y_continuous( label = percent ) +
ggtitle( "Train/Test Precision vs Cutoff" )+
ylab("Precision")
return( list( data = accuracy, plot = plot ) )
}
PrecisionCutoffInfo <- function( train, test, predict, actual )
{
cutoff <- seq( .01, .9, by = .05 )
accuracy <- lapply( cutoff, function(c)
{
data_train <- as.factor( as.numeric( train[[predict]] > c ) )
cm_train <- confusionMatrix(data_train, as.factor(train[[actual]]),positive="1" )
data_test <- as.factor( as.numeric( test[[predict]] > c ) )
cm_test  <- confusionMatrix( data_test, as.factor(test[[actual]]),positive="1" )
dt <- data.table( cutoff = c,
train  = cm_train$byClass[["Precision"]],
test   = cm_test$byClass[["Precision"]] )
return(dt)
}) %>% rbindlist()
accuracy_long <- gather( accuracy, "data", "accuracy", -1 )
plot <- ggplot( accuracy_long, aes( cutoff, accuracy, group = data, color = data ) ) +
geom_line( size = 1 ) + geom_point( size = 3 ) +
scale_y_continuous( label = percent ) +
ggtitle( "Train/Test Precision vs Cutoff" )+
ylab("Precision")
return( list( data = accuracy, plot = plot ) )
}
par(mfrow = c(1, 2))
list1 <- PrecisionCutoffInfo( train = beforeSMOTE, test = test, predict = "prediction", actual = "stroke" )
list2<- PrecisionCutoffInfo( train = afterSMOTE, test = testAfterSMOTE, predict = "prediction", actual = "stroke" )
ggp1 <- list1$plot
ggp2 <- list2$plot
grid.arrange(ggp1, ggp2, ncol = 2)
FOneCutoffInfo <- function( train, test, predict, actual )
{
cutoff <- seq( .01, .9, by = .05 )
accuracy <- lapply( cutoff, function(c)
{
data_train <- as.factor( as.numeric( train[[predict]] > c ) )
cm_train <- confusionMatrix(data_train, as.factor(train[[actual]]),positive="1" )
data_test <- as.factor( as.numeric( test[[predict]] > c ) )
cm_test  <- confusionMatrix( data_test, as.factor(test[[actual]]),positive="1" )
dt <- data.table( cutoff = c,
train  = cm_train$byClass[["F1"]],
test   = cm_test$byClass[["F1"]] )
return(dt)
}) %>% rbindlist()
accuracy_long <- gather( accuracy, "data", "accuracy", -1 )
plot <- ggplot( accuracy_long, aes( cutoff, accuracy, group = data, color = data ) ) +
geom_line( size = 1 ) + geom_point( size = 3 ) +
scale_y_continuous( label = percent ) +
ggtitle( "Train/Test F1 vs Cutoff" )+
ylab("F1 Score")
return( list( data = accuracy, plot = plot ) )
}
par(mfrow = c(1, 2))
list1 <- FOneCutoffInfo( train = beforeSMOTE, test = test, predict = "prediction", actual = "stroke" )
list2<- FOneCutoffInfo( train = afterSMOTE, test = testAfterSMOTE, predict = "prediction", actual = "stroke" )
ggp1 <- list1$plot
ggp2 <- list2$plot
grid.arrange(ggp1, ggp2, ncol = 2)
c_b <- 0.02
beforeSMOTEpred <- as.factor( as.numeric( test[["prediction"]] > c_b ) )
cm_test_b <- confusionMatrix(beforeSMOTEpred, as.factor(test$stroke),positive="1" )
cm_test_b
# loading the package
library(pROC)
#loadPkg("pROC") # receiver operating characteristic curve, gives the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The curve is on sensitivity/recall/true-positive-rate vs false_alarm/false-positive-rate/fall-out.
prob=predict(logitModel_b,test, type = "response")
#Admit$prob=prob
h <- roc(stroke~prob, data=test,levels = c(1, 0))
auc(h) # area-under-curve prefer 0.8 or higher.
plot(h,main="ROC-plot Before SMOTE")
prob=predict(logitModel_a,testAfterSMOTE, type = "response")
#Admit$prob=prob
h_logistic <- roc(stroke~prob, data=testAfterSMOTE)
auc(h_logistic) # area-under-curve prefer 0.8 or higher.
plot(h_logistic,main="ROC plot- After SMOTE")
testDataAfterSmote <- SMOTE(stroke ~ ., test, perc.over = 2000, perc.under = 100)
test_0 <- subset(testDataAfterSmote,testDataAfterSmote$stroke==0)
test_1 <- subset(testDataAfterSmote,testDataAfterSmote$stroke==1)
print(nrow(test_1))
print(nrow(test_0))
testDataAfterSmote$glucoseGroup=cut(testDataAfterSmote$avg_glucose_level, c(0, 114, 140, Inf), c("Normal", "Prediabetes", "Diabetes"), include.lowest=TRUE)
# Predict
pred = predict(logitModel_a,testDataAfterSmote, type = "response")
# Store precision and recall scores at different cutoffs
library(ROCR)
predobj <- prediction(pred, testDataAfterSmote$stroke)
perf <- performance(predobj,"prec", "rec")
ggp2<-plot(perf,main="PR curve -- test Data After SMOTE")
#prob=predict(logitModel_a,testAfterSMOTE, type = "response")
#Admit$prob=prob
h <- roc(stroke~pred, data=testDataAfterSmote)
auc(h) # area-under-curve prefer 0.8 or higher.
plot(h,main="ROC curve -- test Data After SMOTE")
plot(ROC_rf, col = "green", main = "ROC For all the three models")
plot(h, add=TRUE, col="red")
plot(h_logistic, add=TRUE, col='blue')
legend(0.3,0.4,legend=c("Random Forest", "Logistic Regression", "KNN"),
fill = c("green","blue", "red"))
plot(ROC_rf, col = "green", main = "ROC For all the three models")
plot(h, add=TRUE, col="red")
plot(h_logistic, add=TRUE, col='blue')
legend(0.3,0.4,legend=c("Random Forest", "KNN", "Logistic Regression"),
fill = c("green","blue", "red"))
# For KNN Model for After SMOTE
h_KNN <- roc(split1.testLabels, attributes(pred_best)$prob)
auc(h_KNN) # area-under-curve prefer 0.8 or higher.
plot(h_KNN)
plot(ROC_rf, col = "green", main = "ROC For all the three models")
plot(h_KNN, add=TRUE, col="red")
plot(h_logistic, add=TRUE, col='blue')
legend(0.3,0.4,legend=c("Random Forest", "KNN", "Logistic Regression"),
fill = c("green","blue", "red"))
plot(ROC_rf, col = "green", main = "ROC For all the three models")
plot(h_KNN, add=TRUE, col="red")
plot(h_logistic, add=TRUE, col='blue')
legend(0.3,0.4,legend=c("Random Forest", "Logistic Regression","KNN" ),
fill = c("green","blue", "red"))
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library(caret)
library(dplyr)
library("pROC")
library(randomForest)
library(lessR)
#install.packages("gridExtra")
library("gridExtra")
library(tidyr)
# Useful functions when working with logistic regression
library(ROCR)
library(grid)
library(caret)
library(scales)
library(ggplot2)
library(gridExtra)
library(data.table)
library(caret)
library(pscl)
library(DMwR)
##Loading both the datasets
beforeSMOTE <- data.frame(read.csv("trainDataBeforeSmote.csv"))
afterSMOTE <- data.frame(read.csv("trainDataAfterSmote.csv"))
test <- data.frame(read.csv("test.csv"))
##Factoring the Categorical variables as before
beforeSMOTE$gender = factor(beforeSMOTE$gender)
beforeSMOTE$hypertension = factor(beforeSMOTE$hypertension)
beforeSMOTE$heart_disease = factor(beforeSMOTE$heart_disease)
beforeSMOTE$ever_married = factor(beforeSMOTE$ever_married)
beforeSMOTE$work_type = factor(beforeSMOTE$work_type)
beforeSMOTE$Residence_type = factor(beforeSMOTE$Residence_type)
beforeSMOTE$glucoseGroup = factor(beforeSMOTE$glucoseGroup)
beforeSMOTE$smoking_status = factor(beforeSMOTE$smoking_status)
beforeSMOTE$stroke = factor(beforeSMOTE$stroke)
summary(beforeSMOTE)
#factoring the dataset
afterSMOTE$gender = factor(afterSMOTE$gender)
afterSMOTE$hypertension = factor(afterSMOTE$hypertension)
afterSMOTE$heart_disease = factor(afterSMOTE$heart_disease)
afterSMOTE$ever_married = factor(afterSMOTE$ever_married)
afterSMOTE$work_type = factor(afterSMOTE$work_type)
afterSMOTE$Residence_type = factor(afterSMOTE$Residence_type)
afterSMOTE$smoking_status = factor(afterSMOTE$smoking_status)
afterSMOTE$stroke = factor(afterSMOTE$stroke)
afterSMOTE$glucoseGroup = factor(afterSMOTE$glucoseGroup)
summary(afterSMOTE)
#testing
#factoring the dataset
test$gender = factor(test$gender)
test$hypertension = factor(test$hypertension)
test$heart_disease = factor(test$heart_disease)
test$ever_married = factor(test$ever_married)
test$work_type = factor(test$work_type)
test$Residence_type = factor(test$Residence_type)
test$smoking_status = factor(test$smoking_status)
test$stroke = factor(test$stroke)
test$glucoseGroup = factor(test$glucoseGroup)
summary(test)
test_0 <- subset(test,test$stroke==0)
test_1 <- subset(test,test$stroke==1)
train_0 <- subset(beforeSMOTE,beforeSMOTE$stroke==0)
train_1 <- subset(beforeSMOTE,beforeSMOTE$stroke==1)
train_0a <- subset(afterSMOTE,afterSMOTE$stroke==0)
train_1a <- subset(afterSMOTE,afterSMOTE$stroke==1)
# cols <-  hcl.colors(length(levels(afterSMOTE$stroke)), "Fall")
# PieChart(stroke, data = afterSMOTE, hole = 0,
#          fill = cols,
#          labels_cex = 0.6)
#set it to beforeSMOTE
modellingDataset1 <- beforeSMOTE
head(modellingDataset1)
# convert categorical variables to numerical except our response variable
data <- dummyVars("stroke ~ .", data = modellingDataset1, fullRank = T)
custom_train_data1 <- data.frame(predict(data, newdata = modellingDataset1))
test_data <- dummyVars("stroke ~ .", data = test, fullRank = T)
custom_test_data1 <- data.frame(predict(test_data, newdata = test))
length(custom_train_data1)
head(custom_train_data1)
library(dplyr)
set.seed(123)
split1<- sample(c(rep(0, 1.0 * nrow(custom_train_data1))))
split1.trainLabels <- modellingDataset1[split1==0, 11]
split1.testLabels <- test[, 11]
library(ezids)
library(FNN)
library(gmodels)
library(caret)
ResultDf = data.frame( k=numeric(0), Total.Accuracy= numeric(0), row.names = NULL )
for (kval in 3:15) {
prediction <- knn(train = custom_train_data1, test = custom_test_data1, cl=split1.trainLabels, k=kval)
cm = confusionMatrix(prediction, reference = split1.testLabels )
cmaccu = cm$overall['Accuracy']
cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL )
ResultDf = rbind(ResultDf, cmt)
}
xkabledply(ResultDf, "Total Accuracy Summary")
best_accuracy <- ResultDf[which.max(ResultDf$Total.Accuracy),]
k_value <- best_accuracy$k
pred_best <- knn(train =custom_train_data1, test = custom_test_data1, cl=split1.trainLabels, k=k_value, prob=TRUE)
PREDCross_best <- CrossTable(split1.testLabels, pred_best, prop.chisq = FALSE)
cm_best = confusionMatrix(pred_best, reference = split1.testLabels )
xkabledply(as.matrix(cm_best), title = paste("ConfusionMatrix for k = ",k_value))
xkabledply(data.frame(cm_best$byClass), title=paste("k = ",k_value))
# For KNN Model for Before SMOTE
h <- roc(split1.testLabels, attributes(pred_best)$prob)
auc(h) # area-under-curve prefer 0.8 or higher.
plot(h)
#set it to afterSMOTE
modellingDataset2 <- afterSMOTE
modellingDataset2 <- modellingDataset2 %>% relocate(stroke, .after=glucoseGroup)
head(modellingDataset2)
# convert categorical variables to numerical except our response variable
data2 <- dummyVars("stroke ~ .", data = modellingDataset2, fullRank = T)
custom_train_data2 <- data.frame(predict(data2, newdata = modellingDataset2))
length(custom_train_data2)
head(custom_train_data2)
library(dplyr)
set.seed(123)
split2<- sample(c(rep(0, 1.0 * nrow(custom_train_data2))))
split2.trainLabels <- modellingDataset2[split2==0, 12]
library(FNN)
library(gmodels)
library(caret)
ResultDf = data.frame( k=numeric(0), Total.Accuracy= numeric(0), row.names = NULL )
for (kval in 3:15) {
prediction <- knn(train = custom_train_data2, test = custom_test_data1, cl=split2.trainLabels, k=kval)
cm = confusionMatrix(prediction, reference = split1.testLabels )
cmaccu = cm$overall['Accuracy']
cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL )
ResultDf = rbind(ResultDf, cmt)
}
xkabledply(ResultDf, "Total Accuracy Summary")
best_accuracy <- ResultDf[which.max(ResultDf$Total.Accuracy),]
k_value <- best_accuracy$k
pred_best <- knn(train = custom_train_data2, test = custom_test_data1, cl=split2.trainLabels, k=k_value, prob=TRUE)
PREDCross_best <- CrossTable(split1.testLabels, pred_best, prop.chisq = FALSE)
cm_best = confusionMatrix(pred_best, reference = split1.testLabels )
xkabledply(as.matrix(cm_best), title = paste("ConfusionMatrix for k = ",k_value))
xkabledply(data.frame(cm_best$byClass), title=paste("k = ",k_value))
# For KNN Model for After SMOTE
h_KNN <- roc(split1.testLabels, attributes(pred_best)$prob)
auc(h_KNN) # area-under-curve prefer 0.8 or higher.
plot(h_KNN)
chooseK = function(k, train_set, val_set, train_class, val_class){
# Build knn with k neighbors considered.
set.seed(1)
class_knn = knn(train = train_set,    #<- training set cases
test = val_set,       #<- test set cases
cl = train_class,     #<- category for classification
k = k) #,                #<- number of neighbors considered
# use.all = TRUE)       #<- control ties between class assignments. If true, all distances equal to the k-th largest are included
tab = table(class_knn, val_class)
# Calculate the accuracy.
accu = sum(tab[row(tab) == col(tab)]) / sum(tab)
cbind(k = k, accuracy = accu)
}
# The sapply() function plugs in several values into our chooseK function.
# function(x)[function] allows you to apply a series of numbers
# to a function without running a for() loop.
knn_different_k = sapply(seq(1, 21, by = 2),  #<- set k to be odd number from 1 to 21
function(x) chooseK(x,
train_set = custom_train_data2,
val_set = custom_test_data1,
train_class = modellingDataset2[split2==0, 12],
val_class = test[, 11]))
# Reformat the results to graph the results.
str(knn_different_k)
knn_different_k = data.frame(k = knn_different_k[1,],
accuracy = knn_different_k[2,])
# Plot accuracy vs. k.
# install.packages("ggplot2")
loadPkg("ggplot2")
ggplot(knn_different_k, aes(x = k, y = accuracy)) +
geom_line(color = "orange", size = 1.5) +
geom_point(size = 3) +
labs(title = "accuracy vs k")
rfBS <- randomForest(stroke~., data=beforeSMOTE, proximity=TRUE) #beforeSMOTE
