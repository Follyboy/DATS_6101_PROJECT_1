---
title: "SummaryPaper"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

Importing the required libraries
```{r}
library(caret)
library(dplyr)
library("pROC")
library(randomForest)

library(lessR)
#install.packages("gridExtra")              
library("gridExtra")
library(tidyr)
# Useful functions when working with logistic regression
library(ROCR)
library(grid)
library(caret)
library(scales)
library(ggplot2)
library(gridExtra)
library(data.table)
library(caret)
library(pscl)
library(DMwR)
```


##Factoring the Categorical variables as before 
```{r results='markup'}
##Loading both the datasets 
beforeSMOTE <- data.frame(read.csv("trainDataBeforeSmote.csv"))
afterSMOTE <- data.frame(read.csv("trainDataAfterSmote.csv"))
test <- data.frame(read.csv("test.csv"))

##Factoring the Categorical variables as before 
beforeSMOTE$gender = factor(beforeSMOTE$gender)
beforeSMOTE$hypertension = factor(beforeSMOTE$hypertension)
beforeSMOTE$heart_disease = factor(beforeSMOTE$heart_disease)
beforeSMOTE$ever_married = factor(beforeSMOTE$ever_married)
beforeSMOTE$work_type = factor(beforeSMOTE$work_type)
beforeSMOTE$Residence_type = factor(beforeSMOTE$Residence_type)
beforeSMOTE$glucoseGroup = factor(beforeSMOTE$glucoseGroup)
beforeSMOTE$smoking_status = factor(beforeSMOTE$smoking_status)
beforeSMOTE$stroke = factor(beforeSMOTE$stroke)
summary(beforeSMOTE)

#factoring the dataset
afterSMOTE$gender = factor(afterSMOTE$gender)
afterSMOTE$hypertension = factor(afterSMOTE$hypertension)
afterSMOTE$heart_disease = factor(afterSMOTE$heart_disease)
afterSMOTE$ever_married = factor(afterSMOTE$ever_married)
afterSMOTE$work_type = factor(afterSMOTE$work_type)
afterSMOTE$Residence_type = factor(afterSMOTE$Residence_type)
afterSMOTE$smoking_status = factor(afterSMOTE$smoking_status)
afterSMOTE$stroke = factor(afterSMOTE$stroke)
afterSMOTE$glucoseGroup = factor(afterSMOTE$glucoseGroup)
summary(afterSMOTE)

#testing 
#factoring the dataset
test$gender = factor(test$gender)
test$hypertension = factor(test$hypertension)
test$heart_disease = factor(test$heart_disease)
test$ever_married = factor(test$ever_married)
test$work_type = factor(test$work_type)
test$Residence_type = factor(test$Residence_type)
test$smoking_status = factor(test$smoking_status)
test$stroke = factor(test$stroke)
test$glucoseGroup = factor(test$glucoseGroup)
summary(test)
```

## number of rows in test dataset
```{r}
test_0 <- subset(test,test$stroke==0)
test_1 <- subset(test,test$stroke==1)

train_0 <- subset(beforeSMOTE,beforeSMOTE$stroke==0)
train_1 <- subset(beforeSMOTE,beforeSMOTE$stroke==1)

train_0a <- subset(afterSMOTE,afterSMOTE$stroke==0)
train_1a <- subset(afterSMOTE,afterSMOTE$stroke==1)

```

## Setting the variable for checking  for Before SMOTE
```{r}
#set it to beforeSMOTE

modellingDataset1 <- beforeSMOTE

# convert categorical variables to numerical except our response variable
data <- dummyVars("stroke ~ .", data = modellingDataset1, fullRank = T)
custom_train_data1 <- data.frame(predict(data, newdata = modellingDataset1))

test_data <- dummyVars("stroke ~ .", data = test, fullRank = T)
custom_test_data1 <- data.frame(predict(test_data, newdata = test))
```

```{r}
library(dplyr)

set.seed(123)  
split1<- sample(c(rep(0, 1.0 * nrow(custom_train_data1))))


split1.trainLabels <- modellingDataset1[split1==0, 11]
split1.testLabels <- test[, 11]
```

##KNN for Before SMOTE
```{r, results='markup'}
library(ezids)
library(FNN)
library(gmodels)
library(caret)


ResultDf = data.frame( k=numeric(0), Total.Accuracy= numeric(0), row.names = NULL )

for (kval in 3:15) {
  prediction <- knn(train = custom_train_data1, test = custom_test_data1, cl=split1.trainLabels, k=kval)

  cm = confusionMatrix(prediction, reference = split1.testLabels )

  cmaccu = cm$overall['Accuracy']

  cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL )
  ResultDf = rbind(ResultDf, cmt)
}
```

```{r, results='markup'}
# xkabledply(ResultDf, "Total Accuracy Summary")
best_accuracy <- ResultDf[which.max(ResultDf$Total.Accuracy),]
k_value1 <- best_accuracy$k

chooseK = function(k, train_set, val_set, train_class, val_class){
  
  # Build knn with k neighbors considered.
  set.seed(1)
  class_knn = knn(train = train_set,    #<- training set cases
                  test = val_set,       #<- test set cases
                  cl = train_class,     #<- category for classification
                  k = k) #,                #<- number of neighbors considered
                  # use.all = TRUE)       #<- control ties between class assignments. If true, all distances equal to the k-th largest are included
  
  tab = table(class_knn, val_class)
  
  # Calculate the accuracy.
  accu = sum(tab[row(tab) == col(tab)]) / sum(tab)                         
  cbind(k = k, accuracy = accu)
}

# The sapply() function plugs in several values into our chooseK function.
# function(x)[function] allows you to apply a series of numbers
# to a function without running a for() loop.
knn_different_k = sapply(seq(1, 21, by = 2),  #<- set k to be odd number from 1 to 21
                         function(x) chooseK(x, 
                                             train_set = custom_train_data1,
                                             val_set = custom_test_data1,
                                             train_class = modellingDataset1[split1==0, 11],
                                             val_class = test[, 11]))

# Reformat the results to graph the results.
# str(knn_different_k)
knn_different_k = data.frame(k = knn_different_k[1,],
                             accuracy = knn_different_k[2,])

# Plot accuracy vs. k.
# install.packages("ggplot2")
loadPkg("ggplot2")

ggplot(knn_different_k, aes(x = k, y = accuracy)) +
  geom_line(color = "orange", size = 1.5) +
  geom_point(size = 3) + 
  labs(title = "accuracy vs k")


```

* According to the accuracy graph using k-values between 1 to 21 for our original data, our best accuracy model is with a k-value of `k_value1`. But looking at it, we have multiple values which would give us an accurate model, this is due to the fact that over 98% of our data set are made up of individuals that are not prone to stroke.

```{r, results='markup'}
pred_best <- knn(train =custom_train_data1, test = custom_test_data1, cl=split1.trainLabels, k=k_value, prob=TRUE)
cm_best = confusionMatrix(pred_best, reference = split1.testLabels )
xkabledply(as.matrix(cm_best), title = paste("ConfusionMatrix for k = ",k_value))
```

* After plotting a confusion matrix for our original data set, we noticed we had no predictions of individuals that would be prone to stroke which didn't help our research. But the accuracy of the model being `0.9798`, meant that the model was 97.98% accurate.


```{r, results='markup'}
# For KNN Model for Before SMOTE
h <- roc(split1.testLabels, attributes(pred_best)$prob)
plot(h)
```

* After plotting the ROC graph, we see the area-under-curve of `auc(h)`, which is less than 0.8. This test shows that this model is not considered a good fit.


## Setting the variable for checking 
```{r, results='markup'}
#set it to afterSMOTE

modellingDataset2 <- afterSMOTE
modellingDataset2 <- modellingDataset2 %>% relocate(stroke, .after=glucoseGroup)

# convert categorical variables to numerical except our response variable
data2 <- dummyVars("stroke ~ .", data = modellingDataset2, fullRank = T)
custom_train_data2 <- data.frame(predict(data2, newdata = modellingDataset2))
```


## Splitting the dataset into train and test for After SMOTE
```{r}
library(dplyr)

set.seed(123)  
split2<- sample(c(rep(0, 1.0 * nrow(custom_train_data2))))

split2.trainLabels <- modellingDataset2[split2==0, 12]
```

```{r, results='markup'}

library(FNN)
library(gmodels)
library(caret)


ResultDf = data.frame( k=numeric(0), Total.Accuracy= numeric(0), row.names = NULL )

for (kval in 3:15) {
  prediction <- knn(train = custom_train_data2, test = custom_test_data1, cl=split2.trainLabels, k=kval)

  cm = confusionMatrix(prediction, reference = split1.testLabels )

  cmaccu = cm$overall['Accuracy']

  cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL )
  ResultDf = rbind(ResultDf, cmt)
}

best_accuracy <- ResultDf[which.max(ResultDf$Total.Accuracy),]
k_value <- best_accuracy$k

pred_best <- knn(train = custom_train_data2, test = custom_test_data1, cl=split2.trainLabels, k=k_value, prob=TRUE)
cm_best = confusionMatrix(pred_best, reference = split1.testLabels )

xkabledply(as.matrix(cm_best), title = paste("ConfusionMatrix for k = ",k_value))

```


* After plotting a confusion matrix for our data set after using the SMOTE technique, we noticed we had better true value predictions of individuals that would be prone to stroke which helped our research a little bit more in order to better predict individuals prone to stroke. But the accuracy of the model being `0.873`, meant that the model was 87.38% accurate.

```{r, results='markup'}
chooseK = function(k, train_set, val_set, train_class, val_class){
  
  # Build knn with k neighbors considered.
  set.seed(1)
  class_knn = knn(train = train_set,    #<- training set cases
                  test = val_set,       #<- test set cases
                  cl = train_class,     #<- category for classification
                  k = k) #,                #<- number of neighbors considered
                  # use.all = TRUE)       #<- control ties between class assignments. If true, all distances equal to the k-th largest are included
  
  tab = table(class_knn, val_class)
  
  # Calculate the accuracy.
  accu = sum(tab[row(tab) == col(tab)]) / sum(tab)                         
  cbind(k = k, accuracy = accu)
}

# The sapply() function plugs in several values into our chooseK function.
# function(x)[function] allows you to apply a series of numbers
# to a function without running a for() loop.
knn_different_k = sapply(seq(1, 21, by = 2),  #<- set k to be odd number from 1 to 21
                         function(x) chooseK(x, 
                                             train_set = custom_train_data2,
                                             val_set = custom_test_data1,
                                             train_class = modellingDataset2[split2==0, 12],
                                             val_class = test[, 11]))

knn_different_k = data.frame(k = knn_different_k[1,],
                             accuracy = knn_different_k[2,])

# Plot accuracy vs. k.
# install.packages("ggplot2")
loadPkg("ggplot2")

ggplot(knn_different_k, aes(x = k, y = accuracy)) +
  geom_line(color = "orange", size = 1.5) +
  geom_point(size = 3) + 
  labs(title = "accuracy vs k")


``` 


* According to the accuracy graph using k-values between 1 to 21 for our data set after using the SMOTE technique, our best accuracy model is with a k-value of `k_value`. Looking at this graph, we have a better visualization of the value which would give us an accurate model than our previous accuracy graph, this is because the SMOTE technique improved our data set by giving us more predictions on individuals that are prone to stroke, it did not give us a lot of true positive values, but it improved our results.


```{r, results='markup'}
# For KNN Model for After SMOTE
h <- roc(split1.testLabels, attributes(pred_best)$prob)
plot(h)
```
*After plotting the ROC graph, we see the area-under-curve of `auc(h)`, which is less than 0.8. This graph shows that this model is not considered a good fit but it is way better than the previous model that used our original data before using our SMOTE technique.