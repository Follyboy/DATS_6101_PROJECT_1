---
title: "SummaryPaper"
author: "Team 6"
date: "2022-12-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Importing the required libraries
```{r}
library(caret)
library(dplyr)
library(lessR)
#install.packages("gridExtra")              
library("gridExtra")
library(tidyr)
# Useful functions when working with logistic regression
library(ROCR)
library(grid)
library(caret)
library(dplyr)
library(scales)
library(ggplot2)
library(gridExtra)
library(data.table)
library(caret)
library(pscl)
library(DMwR)
```

##Loading both the datasets 
```{r results='markup'}
beforeSMOTE <- data.frame(read.csv("trainDataBeforeSmote.csv"))
afterSMOTE <- data.frame(read.csv("trainDataAfterSmote.csv"))
test <- data.frame(read.csv("test.csv"))

##Factoring the Categorical variables as before 
beforeSMOTE$gender = factor(beforeSMOTE$gender)
beforeSMOTE$hypertension = factor(beforeSMOTE$hypertension)
beforeSMOTE$heart_disease = factor(beforeSMOTE$heart_disease)
beforeSMOTE$ever_married = factor(beforeSMOTE$ever_married)
beforeSMOTE$work_type = factor(beforeSMOTE$work_type)
beforeSMOTE$Residence_type = factor(beforeSMOTE$Residence_type)
beforeSMOTE$glucoseGroup = factor(beforeSMOTE$glucoseGroup)
beforeSMOTE$smoking_status = factor(beforeSMOTE$smoking_status)
beforeSMOTE$stroke = factor(beforeSMOTE$stroke)
#summary(beforeSMOTE)

#factoring the dataset
afterSMOTE$gender = factor(afterSMOTE$gender)
afterSMOTE$hypertension = factor(afterSMOTE$hypertension)
afterSMOTE$heart_disease = factor(afterSMOTE$heart_disease)
afterSMOTE$ever_married = factor(afterSMOTE$ever_married)
afterSMOTE$work_type = factor(afterSMOTE$work_type)
afterSMOTE$Residence_type = factor(afterSMOTE$Residence_type)
afterSMOTE$smoking_status = factor(afterSMOTE$smoking_status)
afterSMOTE$stroke = factor(afterSMOTE$stroke)
afterSMOTE$glucoseGroup = factor(afterSMOTE$glucoseGroup)
#summary(afterSMOTE)

#testing 
#factoring the dataset
test$gender = factor(test$gender)
test$hypertension = factor(test$hypertension)
test$heart_disease = factor(test$heart_disease)
test$ever_married = factor(test$ever_married)
test$work_type = factor(test$work_type)
test$Residence_type = factor(test$Residence_type)
test$smoking_status = factor(test$smoking_status)
test$stroke = factor(test$stroke)
test$glucoseGroup = factor(test$glucoseGroup)
#summary(test)

testAfterSMOTE <- test 
cols <-  hcl.colors(length(levels(beforeSMOTE$stroke)), "Fall")
PieChart(stroke, data = beforeSMOTE, hole = 0,
         fill = cols,
         labels_cex = 0.6)

```
```{r results='markup'}
### After SMOTE
cols <-  hcl.colors(length(levels(afterSMOTE$stroke)), "Fall")
PieChart(stroke, data = afterSMOTE, hole = 0,
         fill = cols,
         labels_cex = 0.6)
```

## number of rows in test dataset
```{r}
test_0 <- subset(test,test$stroke==0)
test_1 <- subset(test,test$stroke==1)
print(nrow(test_1))
print(nrow(test_0))


train_0 <- subset(beforeSMOTE,beforeSMOTE$stroke==0)
train_1 <- subset(beforeSMOTE,beforeSMOTE$stroke==1)
print(nrow(train_0))
print(nrow(train_1))


train_0a <- subset(afterSMOTE,afterSMOTE$stroke==0)
train_1a <- subset(afterSMOTE,afterSMOTE$stroke==1)
print(nrow(train_0a))
print(nrow(train_1a))
```
Testing set 

* `r nrow(test_1)` entries as 1.
* `r nrow(test_0)` entries as 0.

Training Before SMOTE

* `r nrow(train_1)` entries as 1.
* `r nrow(train_0)` entries as 0.

Training After SMOTE

* `r nrow(train_1a)` entries as 1.
* `r nrow(train_0a)` entries as 0.

## Logistic Regression on Original Dataset 
```{r}
#before smote training
logitModel1_b <- glm(stroke~ ., data = beforeSMOTE, family = binomial())
logitModel2_b <- glm(stroke~ age + age:hypertension + age:heart_disease +age:avg_glucose_level, data = beforeSMOTE, family = binomial())
logitModel3_b <- glm(stroke~  age*heart_disease +age*avg_glucose_level, data = beforeSMOTE, family = binomial())
logitModel4_b <- glm(stroke~ (age+heart_disease+ glucoseGroup)^2 , data = beforeSMOTE, family = binomial())
logitModel5_b <- glm(stroke~ (age + glucoseGroup)^2 , data = beforeSMOTE, family = binomial())

#after smote training
logitModel1_a <- glm(stroke~ ., data = afterSMOTE, family = binomial())
logitModel2_a <- glm(stroke~ age + age:hypertension + age:heart_disease +age:avg_glucose_level, data = afterSMOTE, family = binomial())
logitModel3_a <- glm(stroke~ age*heart_disease +age*avg_glucose_level, data = afterSMOTE, family = binomial())
logitModel4_a <- glm(stroke~ (age+heart_disease+ glucoseGroup)^2, data = afterSMOTE, family = binomial())
logitModel5_a <- glm(stroke~ (age + glucoseGroup)^2, data = afterSMOTE, family = binomial())

#selecting model for furter analysis of code
logitModel_b <-logitModel5_b
logitModel_a <- logitModel5_a
```


```{r results='markup'}
 anova(logitModel1_b, logitModel2_b,logitModel3_b,logitModel4_b,logitModel5_b,test="Chisq")
```


```{r results='markup'}
summary(logitModel_b)
summary(logitModel_a)
```

## Before SMOTE
```{r results='markup'}
pR2(logitModel_b)
AIC(logitModel_b)
```

## After SMOTE
```{r results='markup'}
pR2(logitModel_a)
AIC(logitModel_a)
```



## Plotting Training Set Prediction Score
Before SMOTE
```{r results='markup'}

beforeSMOTE$prediction <- predict( logitModel_b, newdata = beforeSMOTE, type = "response" )
test$prediction  <- predict( logitModel_b, newdata = test , type = "response" )

par(mfrow = c(1, 2))
ggp1 <- ggplot( beforeSMOTE, aes( prediction, color = as.factor(stroke) ) ) + 
geom_density( size = 1 ) +
ggtitle( "Training Set's Predicted Score before SMOTE" ) 

ggp2 <- ggplot( test, aes( prediction, color = as.factor(stroke) ) ) + 
geom_density( size = 1 ) +
ggtitle( "Testing Set's Predicted Score before SMOTE" ) 
grid.arrange(ggp1, ggp2, ncol = 2) 
```
Now for before smote the cut-off value is very close to 0. Mostly between 0.0 to 0.05.
But the reason this is not an accurate estimate is because of the difference in size of 0 and 1 enteries. 
even though the density is high for blue curve after the intersection point they are relative to the number of enteries in each dataset. 

##After SMOTE

Here the optimal cut off can be the 
```{r results='markup'}

afterSMOTE$prediction <- predict( logitModel_a, newdata = afterSMOTE, type = "response" )
testAfterSMOTE$prediction  <- predict( logitModel_a, newdata = test , type = "response" )
par(mfrow = c(1, 2))
ggp1 <- ggplot( afterSMOTE, aes( prediction, color = as.factor(stroke) ) ) + 
geom_density( size = 1 ) +
ggtitle( "Training Set's Predicted Score after SMOTE" ) 

ggp2 <- ggplot( testAfterSMOTE, aes( prediction, color = as.factor(stroke) ) ) + 
geom_density( size = 1 ) +
ggtitle( "Testing Set's Predicted Score after SMOTE" ) 
grid.arrange(ggp1, ggp2, ncol = 2) 
```
So as observed there is a slight shift in intersection point for cut off value. 
which is because after training on more data, it has learned better and started assigning greater probabilities to the 1 class. 


### Confusion Matrix plots at different accuracy

```{r}
ConfusionMatrixInfo <- function( data, predict, actual, cutoff )
{	
	predict <- data[[predict]]
	actual  <- relevel( as.factor( data[[actual]] ), "1" )
	
	result <- data.table( actual = actual, predict = predict )
	result[ , type := ifelse( predict >= cutoff & actual == 1, "TP",
					  ifelse( predict >= cutoff & actual == 0, "FP", 
					  ifelse( predict <  cutoff & actual == 1, "FN", "TN" ) ) ) %>% as.factor() ]

	# jittering : can spread the points along the x axis 
	plot <- ggplot( result, aes( actual, predict, color = type ) ) + 
			geom_violin( fill = "white", color = NA ) +
			geom_jitter( shape = 1 ) + 
			geom_hline( yintercept = cutoff, color = "blue", alpha = 0.6 ) + 
			scale_y_continuous( limits = c( 0, 1 ) ) + 
			scale_color_discrete( breaks = c( "TP", "FN", "FP", "TN" ) ) + # ordering of the legend 
			guides( col = guide_legend( nrow = 2 ) ) + # adjust the legend to have two rows  
			ggtitle( sprintf( "ConfusionMatrix Cutoff : %.2f", cutoff ) )

	return( list( data = result, plot = plot ))
}

```


```{r results='markup'}

# visualize .6 cutoff (lowest point of the previous plot)
# cm_info <- ConfusionMatrixInfo( data = test, predict = "prediction", 
#                                 actual = "stroke", cutoff = .08 )
# #ggthemr("flat")
# cm_info$plot
par(mfrow = c(1, 2))
list1 <- ConfusionMatrixInfo( data = test, predict = "prediction", actual = "stroke", cutoff = .025 )
list2 <- ConfusionMatrixInfo( data = testAfterSMOTE, predict = "prediction", actual = "stroke", cutoff = .37 )
ggp1 <- list1$plot
ggp2 <- list2$plot
grid.arrange(ggp1, ggp2, ncol = 2)


```

### Accuracy versus Cutoff 

```{r results='markup'}
# ------------------------------------------------------------------------------------------
# [AccuracyCutoffInfo] : 
# Obtain the accuracy on the trainining and testing dataset.
# for cutoff value ranging from .4 to .8 ( with a .05 increase )
# @train   : your data.table or data.frame type training data ( assumes you have the predicted score in it ).
# @test    : your data.table or data.frame type testing data
# @predict : prediction's column name (assumes the same for training and testing set)
# @actual  : actual results' column name
# returns  : 1. data : a data.table with three columns.
#            		   each row indicates the cutoff value and the accuracy for the 
#            		   train and test set respectively.
# 			 2. plot : plot that visualizes the data.table
AccuracyCutoffInfo <- function( train, test, predict, actual )
{ 
	cutoff <- seq( .01, .9, by = .05 )
	accuracy <- lapply( cutoff, function(c)
	{
	  data_train <- as.factor( as.numeric( train[[predict]] > c ) )
		cm_train <- confusionMatrix(data_train, as.factor(train[[actual]]),positive="1" )
		data_test <- as.factor( as.numeric( test[[predict]] > c ) )
		cm_test  <- confusionMatrix( data_test, as.factor(test[[actual]]),positive="1" )
			
		dt <- data.table( cutoff = c,
						  train  = cm_train$overall[["Accuracy"]],
		 			    test   = cm_test$overall[["Accuracy"]] )
		return(dt)
	}) %>% rbindlist()

	# visualize the accuracy of the train and test set for different cutoff value 
	# accuracy in percentage.
	accuracy_long <- gather( accuracy, "data", "accuracy", -1 )
	plot <- ggplot( accuracy_long, aes( cutoff, accuracy, group = data, color = data ) ) + 
			geom_line( size = 1 ) + geom_point( size = 3 ) +
			scale_y_continuous( label = percent ) +
			ggtitle( "Train/Test Accuracy vs Cutoff" )
	return( list( data = accuracy, plot = plot ) )
}
```


```{r results='markup'}
par(mfrow = c(1, 2))
list1 <- AccuracyCutoffInfo( train = beforeSMOTE, test = test, predict = "prediction", actual = "stroke" )
list2<- AccuracyCutoffInfo( train = afterSMOTE, test = testAfterSMOTE, predict = "prediction", actual = "stroke" )
ggp1 <- list1$plot
ggp2 <- list2$plot
grid.arrange(ggp1, ggp2, ncol = 2) 
```

Now if observe the accuracy curve,

* For before SMOTE, accuracy keeps on increasing an reaches a plateu after 0.25
if the model classifies all data points as 0, it would achieve a accuracy of 9793/(9793+202) = 97.9%

* For After SMOTE, since for training data we have a significant number of 1 , the probability values are distributed well between 0 to 1. and thus with the increase in cutoff. so the drop in accuracy of training dataset with the increase in cutoff is explained by the fact that a significant numeber of 1's are now wrongly predicted.

* Since testing data is similar, even with the well spreaded values of 0 and 1, even though the density of 1 is more above cutoff, the significantly high number of 0's are overpowering that thing and thus the accuracy keeps on increasing here as well for test dataset. 


#So how should we evaluate our model then? 

So, the above results confirms that in cases with rare disease or imabalance, it is not a good idea to look at accuracy values. 
So instead of that we will focus more on the parameters that are more medically sound.

What do you think can be more important for medical studies?
The answer is I would be more interested in a model which is more sensitive toward people getting stroke. 

The parameters that would consider these things are :

* Sensitivity : Percentage of people with stroke who were correctly identified as getting stroke 

So that more number of people who are going to get stroke can be made caution and protected 

* Precision : the probability that a patient diagnosed as stroke by the classifier got stroke.
So for whatever people the model is predicting as will get stroke, how many of them are actually getting it.
This will make the situation less chaotic among population and make the positive stroke value as more serious. 

Hence we would be more interested in Sensitivity and Precision value.
Let's observe them for our model.

### Sensitivity versus cutoff 
```{r results='markup'}
SensitivityCutoffInfo <- function( train, test, predict, actual )
{
	cutoff <- seq( .01, .9, by = .05 )
	accuracy <- lapply( cutoff, function(c)
	{
	  data_train <- as.factor( as.numeric( train[[predict]] > c ) )
		cm_train <- confusionMatrix(data_train, as.factor(train[[actual]]),positive="1" )
		data_test <- as.factor( as.numeric( test[[predict]] > c ) )
		cm_test  <- confusionMatrix( data_test, as.factor(test[[actual]]),positive="1" )
			
		dt <- data.table( cutoff = c,
						  train  = cm_train$byClass[["Sensitivity"]],
		 			    test   = cm_test$byClass[["Sensitivity"]] )
		return(dt)
	}) %>% rbindlist()
	accuracy_long <- gather( accuracy, "data", "accuracy", -1 )
	
	plot <- ggplot( accuracy_long, aes( cutoff, accuracy, group = data, color = data ) ) + 
			geom_line( size = 1 ) + geom_point( size = 3 ) +
			scale_y_continuous( label = percent ) +
			ggtitle( "Train/Test Sensivity vs Cutoff" )+
	    ylab("Sensitivity")

	return( list( data = accuracy, plot = plot ) )
}
```


```{r results='markup'}
par(mfrow = c(1, 2))
list1 <- SensitivityCutoffInfo( train = beforeSMOTE, test = test, predict = "prediction", actual = "stroke" )
list2<- SensitivityCutoffInfo( train = afterSMOTE, test = testAfterSMOTE, predict = "prediction", actual = "stroke" )
ggp1 <- list1$plot
ggp2 <- list2$plot
grid.arrange(ggp1, ggp2, ncol = 2) 
```
So why is the sensitivity value decreasing so rapidly?
Left curve - before SMOTE
since all the predicted probabilities are between 0&0.25.
after 0 and 0.25, all the datapoints which had stroke = 1, are predicted as 0, the sensitivity is dropping so rapidly. 

Right Curve - after SMOTE
train - since the number of 0 & 1 are more balanced, and the probability values are spreaded well between 0 - 1, the drop in sensitivity value is smooth.

test - the datapoints are now well spreaded between 0 & 1, hence the curve is now smooth 




### Precision versus cutoff 
```{r results='markup'}
PrecisionCutoffInfo <- function( train, test, predict, actual )
{
	cutoff <- seq( .01, .9, by = .05 )
	accuracy <- lapply( cutoff, function(c)
	{
	  data_train <- as.factor( as.numeric( train[[predict]] > c ) )
		cm_train <- confusionMatrix(data_train, as.factor(train[[actual]]),positive="1" )
		data_test <- as.factor( as.numeric( test[[predict]] > c ) )
		cm_test  <- confusionMatrix( data_test, as.factor(test[[actual]]),positive="1" )
			
		dt <- data.table( cutoff = c,
						  train  = cm_train$byClass[["Precision"]],
		 			    test   = cm_test$byClass[["Precision"]] )
		return(dt)
	}) %>% rbindlist()
	accuracy_long <- gather( accuracy, "data", "accuracy", -1 )
	
	plot <- ggplot( accuracy_long, aes( cutoff, accuracy, group = data, color = data ) ) + 
			geom_line( size = 1 ) + geom_point( size = 3 ) +
			scale_y_continuous( label = percent ) +
			ggtitle( "Train/Test Precision vs Cutoff" )+
	    ylab("Precision")

	return( list( data = accuracy, plot = plot ) )
}
```


```{r results='markup'}
par(mfrow = c(1, 2))
list1 <- PrecisionCutoffInfo( train = beforeSMOTE, test = test, predict = "prediction", actual = "stroke" )
list2<- PrecisionCutoffInfo( train = afterSMOTE, test = testAfterSMOTE, predict = "prediction", actual = "stroke" )
ggp1 <- list1$plot
ggp2 <- list2$plot
grid.arrange(ggp1, ggp2, ncol = 2) 
```

Precision is an estimate of how reliable the 1(Stroke) value predicted by model is. 
so now since after the cut off of 0.25, no value is predicted as 1 by the model, FP = 0, and hence, the value is coming out as TP/TP 1.


### F1 Score versus cutoff 

```{r results='markup'}
FOneCutoffInfo <- function( train, test, predict, actual )
{
	cutoff <- seq( .01, .9, by = .05 )
	accuracy <- lapply( cutoff, function(c)
	{
	  data_train <- as.factor( as.numeric( train[[predict]] > c ) )
		cm_train <- confusionMatrix(data_train, as.factor(train[[actual]]),positive="1" )
		data_test <- as.factor( as.numeric( test[[predict]] > c ) )
		cm_test  <- confusionMatrix( data_test, as.factor(test[[actual]]),positive="1" )
			
		dt <- data.table( cutoff = c,
						  train  = cm_train$byClass[["F1"]],
		 			    test   = cm_test$byClass[["F1"]] )
		return(dt)
	}) %>% rbindlist()
	accuracy_long <- gather( accuracy, "data", "accuracy", -1 )
	
	plot <- ggplot( accuracy_long, aes( cutoff, accuracy, group = data, color = data ) ) + 
			geom_line( size = 1 ) + geom_point( size = 3 ) +
			scale_y_continuous( label = percent ) +
			ggtitle( "Train/Test F1 vs Cutoff" )+
	    ylab("F1 Score")

	return( list( data = accuracy, plot = plot ) )
}
```


```{r results='markup'}
par(mfrow = c(1, 2))
list1 <- FOneCutoffInfo( train = beforeSMOTE, test = test, predict = "prediction", actual = "stroke" )
list2<- FOneCutoffInfo( train = afterSMOTE, test = testAfterSMOTE, predict = "prediction", actual = "stroke" )
ggp1 <- list1$plot
ggp2 <- list2$plot
grid.arrange(ggp1, ggp2, ncol = 2) 
```

The higher the precision recall score, the higher would be the F1 score, hence a high F1 score means a better model.

Confusion Matrix at high F1 score

Before SMOTE

```{r results='markup'}
c_b <- 0.025

beforeSMOTEpred <- as.factor( as.numeric( test[["prediction"]] > c_b ) )
cm_test_b <- confusionMatrix(beforeSMOTEpred, as.factor(test$stroke),positive="1" )
cm_test_b

```

* F1 score : `r cm_test_b$byClass[["F1"]]`
* Sensitivity  : `r cm_test_b$byClass[["Sensitivity"]]`
* Precision  : `r cm_test_b$byClass[["Precision"]]`

After SMOTE
```{r results='markup'}
c_a <- 0.25
afterSMOTEpred <- as.factor( as.numeric( testAfterSMOTE[["prediction"]] > c_a ) )
cm_test_a <- confusionMatrix(afterSMOTEpred, as.factor(testAfterSMOTE$stroke),positive="1" )
cm_test_a
```

* F1 score : `r cm_test_a$byClass[["F1"]]` 
* Sensitivity  : `r cm_test_a$byClass[["Sensitivity"]]`
* Precision  : `r cm_test_a$byClass[["Precision"]]`

### Deciding the cut off level 
```{r results}
# setting the cut-off probablity
# function to print confusion matrices for diffrent cut-off levels of probability
#to get confusion matrix checking the max probability from the graph
#for before SMOTE
max_prob_b <- max(test$prediction)
min_prob_b <- min(test$prediction)
jump_b <-  0.01


# making sequence of cut-off probabilities       
cutoff_b <- seq( min_prob_b, max_prob_b, by = jump_b )
CmFn <- function(cutoff,model) {
       # predicting the test set results
         modelPred <- predict(model, test, type = "response")
         C1 <- ifelse(modelPred > cutoff, 1, 0)
         C2 <- test$stroke
         predY   <- as.factor(C1)
         actualY <- as.factor(C2)
        # use the confusionMatrix 
        cm1 <-confusionMatrix(table(predY,actualY),positive="1")
        #print(cutoff)
        #print(cm1$table)
        # extracting accuracy
        Accuracy <- cm1$overall[1]
        #print(Accuracy)
        # extracting sensitivity
          Sensitivity <- cm1$byClass[1]
        # extracting specificity
          Specificity <- cm1$byClass[2]
      # extracting value of Precision
          Precision <- cm1$byClass[5]
      #f1 score
          f1_score <- cm1$byClass[7] 
        # combined table
          tab <- cbind(cutoff,Accuracy,Sensitivity,Specificity,Precision,f1_score)
        return(tab)}

# loop using "lapply"
#tab2    <- lapply(cutoff1, CmFn)
tab_b    <- lapply(cutoff_b, CmFn,logitModel_b)
#tab2 <- CmFn(logitModel1_a,test,cutoff1)
tab_b
       
```

```{r results='markup'}
max_prob_a <- max(testAfterSMOTE)
min_prob_a <- min(testAfterSMOTE$prediction)
jump_a <-  0.1

cutoff_a <- seq( min_prob_a, max_prob_a, by = jump_a )
tab_a <-lapply(cutoff_a, CmFn,logitModel_a)

```


### ROC - AUC Curve 

#ROC Curve
##for balanced dataset
```{r}
# loading the package

library(pROC)
#loadPkg("pROC") # receiver operating characteristic curve, gives the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The curve is on sensitivity/recall/true-positive-rate vs false_alarm/false-positive-rate/fall-out.
prob=predict(logitModel_b,test, type = "response")
#Admit$prob=prob
h <- roc(stroke~prob, data=test,levels = c(1, 0))
auc(h) # area-under-curve prefer 0.8 or higher.
plot(h)
```

##for unbalanced dataset 
```{r results='markup'}
prob=predict(logitModel_a,testAfterSMOTE, type = "response")
#Admit$prob=prob
h <- roc(stroke~prob, data=testAfterSMOTE)
auc(h) # area-under-curve prefer 0.8 or higher.
plot(h)

```

Now why is there no significant difference in the ROC curve value ?
This is because the testing dataset is same for both and the number of values for 1 are significantly small. 



## PR Curve
Always a good idea to plot a precision recall curve when we are more interested in one class out of the two.
Our aim is to make the curve as close to (1, 1) as possible- meaning a good precision and recall

```{r results='markup'}

# Predict
pred = predict(logitModel_b,test, type = "response")
# Store precision and recall scores at different cutoffs
par(mfrow = c(1, 2))
library(ROCR)
predobj <- prediction(pred, test$stroke)
perf <- performance(predobj,"prec", "rec")
ggp1<- plot(perf)

# Predict
pred = predict(logitModel_a,testAfterSMOTE, type = "response")
# Store precision and recall scores at different cutoffs
library(ROCR)
predobj <- prediction(pred, testAfterSMOTE$stroke)
perf <- performance(predobj,"prec", "rec")
ggp2<-plot(perf)



grid.arrange(ggp1, ggp2, ncol = 2) 

```

So the best solution if we generate more enteries in test dataset as well

```{r results='markup'}

testDataAfterSmote <- SMOTE(stroke ~ ., test, perc.over = 2000, perc.under = 100)
test_0 <- subset(testDataAfterSmote,testDataAfterSmote$stroke==0)
test_1 <- subset(testDataAfterSmote,testDataAfterSmote$stroke==1)
print(nrow(test_1))
print(nrow(test_0))

testDataAfterSmote$glucoseGroup=cut(testDataAfterSmote$avg_glucose_level, c(0, 114, 140, Inf), c("Normal", "Prediabetes", "Diabetes"), include.lowest=TRUE)
```


```{r results='markup'}

# Predict
pred = predict(logitModel_a,testDataAfterSmote, type = "response")
# Store precision and recall scores at different cutoffs
library(ROCR)
predobj <- prediction(pred, testDataAfterSmote$stroke)
perf <- performance(predobj,"prec", "rec")
ggp2<-plot(perf)


#prob=predict(logitModel_a,testAfterSMOTE, type = "response")
#Admit$prob=prob
h <- roc(stroke~pred, data=testDataAfterSmote)
auc(h) # area-under-curve prefer 0.8 or higher.
plot(h)
```


Here we can observe the improvement in ROC, AUC=0.88 and PR curve. Since we have significant number of 0 and 1 values.

```{r results='markup'}

list2<- FOneCutoffInfo( train = afterSMOTE, test = testDataAfterSmote, predict = "prediction", actual = "stroke" )
list2$plot
```