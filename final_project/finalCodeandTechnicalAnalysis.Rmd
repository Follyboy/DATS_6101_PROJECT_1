---
title: "finalCodeandTechnicalAnalysis"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

Importing the required libraries
```{r}
library(caret)
library(dplyr)
library("pROC")
library(randomForest)

library(lessR)
#install.packages("gridExtra")              
library("gridExtra")
library(tidyr)
# Useful functions when working with logistic regression
library(ROCR)
library(grid)
library(caret)
library(scales)
library(ggplot2)
library(gridExtra)
library(data.table)
library(caret)
library(pscl)
library(DMwR)
```


##Factoring the Categorical variables as before 
```{r results='markup'}
##Loading both the datasets 
beforeSMOTE <- data.frame(read.csv("trainDataBeforeSmote.csv"))
afterSMOTE <- data.frame(read.csv("trainDataAfterSmote.csv"))
test <- data.frame(read.csv("test.csv"))

##Factoring the Categorical variables as before 
beforeSMOTE$gender = factor(beforeSMOTE$gender)
beforeSMOTE$hypertension = factor(beforeSMOTE$hypertension)
beforeSMOTE$heart_disease = factor(beforeSMOTE$heart_disease)
beforeSMOTE$ever_married = factor(beforeSMOTE$ever_married)
beforeSMOTE$work_type = factor(beforeSMOTE$work_type)
beforeSMOTE$Residence_type = factor(beforeSMOTE$Residence_type)
beforeSMOTE$glucoseGroup = factor(beforeSMOTE$glucoseGroup)
beforeSMOTE$smoking_status = factor(beforeSMOTE$smoking_status)
beforeSMOTE$stroke = factor(beforeSMOTE$stroke)
summary(beforeSMOTE)

#factoring the dataset
afterSMOTE$gender = factor(afterSMOTE$gender)
afterSMOTE$hypertension = factor(afterSMOTE$hypertension)
afterSMOTE$heart_disease = factor(afterSMOTE$heart_disease)
afterSMOTE$ever_married = factor(afterSMOTE$ever_married)
afterSMOTE$work_type = factor(afterSMOTE$work_type)
afterSMOTE$Residence_type = factor(afterSMOTE$Residence_type)
afterSMOTE$smoking_status = factor(afterSMOTE$smoking_status)
afterSMOTE$stroke = factor(afterSMOTE$stroke)
afterSMOTE$glucoseGroup = factor(afterSMOTE$glucoseGroup)
summary(afterSMOTE)

#testing 
#factoring the dataset
test$gender = factor(test$gender)
test$hypertension = factor(test$hypertension)
test$heart_disease = factor(test$heart_disease)
test$ever_married = factor(test$ever_married)
test$work_type = factor(test$work_type)
test$Residence_type = factor(test$Residence_type)
test$smoking_status = factor(test$smoking_status)
test$stroke = factor(test$stroke)
test$glucoseGroup = factor(test$glucoseGroup)
summary(test)
```

## number of rows in test dataset
```{r}
test_0 <- subset(test,test$stroke==0)
test_1 <- subset(test,test$stroke==1)

train_0 <- subset(beforeSMOTE,beforeSMOTE$stroke==0)
train_1 <- subset(beforeSMOTE,beforeSMOTE$stroke==1)

train_0a <- subset(afterSMOTE,afterSMOTE$stroke==0)
train_1a <- subset(afterSMOTE,afterSMOTE$stroke==1)

```
Testing set 

* `r nrow(test_1)` entries as 1.
* `r nrow(test_0)` entries as 0.

Training Before SMOTE

* `r nrow(train_1)` entries as 1.
* `r nrow(train_0)` entries as 0.

Training After SMOTE

* `r nrow(train_1a)` entries as 1.
* `r nrow(train_0a)` entries as 0.


### After SMOTE
```{r}
# cols <-  hcl.colors(length(levels(afterSMOTE$stroke)), "Fall")
# PieChart(stroke, data = afterSMOTE, hole = 0,
#          fill = cols,
#          labels_cex = 0.6)
```

## Setting the variable for checking  for Before SMOTE
```{r}
#set it to beforeSMOTE

modellingDataset1 <- beforeSMOTE

head(modellingDataset1)

# convert categorical variables to numerical except our response variable
data <- dummyVars("stroke ~ .", data = modellingDataset1, fullRank = T)
custom_train_data1 <- data.frame(predict(data, newdata = modellingDataset1))

test_data <- dummyVars("stroke ~ .", data = test, fullRank = T)
custom_test_data1 <- data.frame(predict(test_data, newdata = test))

length(custom_train_data1)
head(custom_train_data1)
```

* We used the `dummyVars` function, which creates dummy variables, to convert our categorical variables to numerical variables in order to run those variables with our `KNN` function.


## Splitting the dataset into train and test for Before SMOTE
```{r}
library(dplyr)

set.seed(123)  
split1<- sample(c(rep(0, 1.0 * nrow(custom_train_data1))))


split1.trainLabels <- modellingDataset1[split1==0, 11]
split1.testLabels <- test[, 11]
```



* We have separated the x variables from our response variable (`stroke`), but before we sparated them, we created a train-test split in a ratio of `7:3` (i.e. 70% training, 30% testing).


##KNN for Before SMOTE
```{r, results='markup'}
library(ezids)
library(FNN)
library(gmodels)
library(caret)


ResultDf = data.frame( k=numeric(0), Total.Accuracy= numeric(0), row.names = NULL )

for (kval in 3:15) {
  prediction <- knn(train = custom_train_data1, test = custom_test_data1, cl=split1.trainLabels, k=kval)

  cm = confusionMatrix(prediction, reference = split1.testLabels )

  cmaccu = cm$overall['Accuracy']

  cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL )
  ResultDf = rbind(ResultDf, cmt)
}

xkabledply(ResultDf, "Total Accuracy Summary")
best_accuracy <- ResultDf[which.max(ResultDf$Total.Accuracy),]
k_value <- best_accuracy$k

pred_best <- knn(train =custom_train_data1, test = custom_test_data1, cl=split1.trainLabels, k=k_value, prob=TRUE)
PREDCross_best <- CrossTable(split1.testLabels, pred_best, prop.chisq = FALSE)
cm_best = confusionMatrix(pred_best, reference = split1.testLabels )

xkabledply(as.matrix(cm_best), title = paste("ConfusionMatrix for k = ",k_value))
xkabledply(data.frame(cm_best$byClass), title=paste("k = ",k_value))

```

* Most of our data before using our sampling technique had result of people without strokes, so this affected our results for our KNN prediction. This is why we do not have predictions of individuals prone to stroke. 

* According to our accuracy summary using k-values between 3 to 15 for our original data, our best accuracy model is with a k-value of `k-value`. 

* From our confusion matrix, we can say that the accuracy of this model is `(9793 + 0)/9995 = 0.9798`, therefore we can say that this model is 97.98% accurate.


```{r, results='markup'}
# For KNN Model for Before SMOTE
h <- roc(split1.testLabels, attributes(pred_best)$prob)
auc(h) # area-under-curve prefer 0.8 or higher.
plot(h)
```

* We have here the area-under-curve of `auc(h)`, which is less than 0.8. This test shows that this model is not considered a good fit.


## Setting the variable for checking 
```{r}
#set it to afterSMOTE

modellingDataset2 <- afterSMOTE
modellingDataset2 <- modellingDataset2 %>% relocate(stroke, .after=glucoseGroup)

head(modellingDataset2)

# convert categorical variables to numerical except our response variable
data2 <- dummyVars("stroke ~ .", data = modellingDataset2, fullRank = T)
custom_train_data2 <- data.frame(predict(data2, newdata = modellingDataset2))

length(custom_train_data2)
head(custom_train_data2)
```


## Splitting the dataset into train and test for After SMOTE
```{r}
library(dplyr)

set.seed(123)  
split2<- sample(c(rep(0, 1.0 * nrow(custom_train_data2))))

split2.trainLabels <- modellingDataset2[split2==0, 12]
```


##KNN for After SMOTE
```{r, results='markup'}

library(FNN)
library(gmodels)
library(caret)


ResultDf = data.frame( k=numeric(0), Total.Accuracy= numeric(0), row.names = NULL )

for (kval in 3:15) {
  prediction <- knn(train = custom_train_data2, test = custom_test_data1, cl=split2.trainLabels, k=kval)

  cm = confusionMatrix(prediction, reference = split1.testLabels )

  cmaccu = cm$overall['Accuracy']

  cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL )
  ResultDf = rbind(ResultDf, cmt)
}

xkabledply(ResultDf, "Total Accuracy Summary")
best_accuracy <- ResultDf[which.max(ResultDf$Total.Accuracy),]
k_value <- best_accuracy$k

pred_best <- knn(train = custom_train_data2, test = custom_test_data1, cl=split2.trainLabels, k=k_value, prob=TRUE)
PREDCross_best <- CrossTable(split1.testLabels, pred_best, prop.chisq = FALSE)
cm_best = confusionMatrix(pred_best, reference = split1.testLabels )

xkabledply(as.matrix(cm_best), title = paste("ConfusionMatrix for k = ",k_value))
xkabledply(data.frame(cm_best$byClass), title=paste("k = ",k_value))

```

* According to our accuracy summary using k-values between 3 to 15 for our original data, our best accuracy model is with a k-value of `k-value`. 

* From our confusion matrix, we can say that the accuracy of this model is `(8407 + 90)/9995 = 0.8501`, therefore we can say that this model is 85.01% accurate.

#Plot ROC-AUC
```{r, results='markup'}
# For KNN Model for After SMOTE
h_KNN <- roc(split1.testLabels, attributes(pred_best)$prob)
auc(h_KNN) # area-under-curve prefer 0.8 or higher.
plot(h_KNN)
```

* We have here the area-under-curve of `auc(h)`, which is less than 0.8. This test shows that this model is not considered a good fit but it is way better than the previous model that used our original data before SMOTE.

```{r}
chooseK = function(k, train_set, val_set, train_class, val_class){
  
  # Build knn with k neighbors considered.
  set.seed(1)
  class_knn = knn(train = train_set,    #<- training set cases
                  test = val_set,       #<- test set cases
                  cl = train_class,     #<- category for classification
                  k = k) #,                #<- number of neighbors considered
                  # use.all = TRUE)       #<- control ties between class assignments. If true, all distances equal to the k-th largest are included
  
  tab = table(class_knn, val_class)
  
  # Calculate the accuracy.
  accu = sum(tab[row(tab) == col(tab)]) / sum(tab)                         
  cbind(k = k, accuracy = accu)
}

# The sapply() function plugs in several values into our chooseK function.
# function(x)[function] allows you to apply a series of numbers
# to a function without running a for() loop.
knn_different_k = sapply(seq(1, 21, by = 2),  #<- set k to be odd number from 1 to 21
                         function(x) chooseK(x, 
                                             train_set = custom_train_data2,
                                             val_set = custom_test_data1,
                                             train_class = modellingDataset2[split2==0, 12],
                                             val_class = test[, 11]))

# Reformat the results to graph the results.
str(knn_different_k)
knn_different_k = data.frame(k = knn_different_k[1,],
                             accuracy = knn_different_k[2,])

# Plot accuracy vs. k.
# install.packages("ggplot2")
loadPkg("ggplot2")

ggplot(knn_different_k, aes(x = k, y = accuracy)) +
  geom_line(color = "orange", size = 1.5) +
  geom_point(size = 3) + 
  labs(title = "accuracy vs k")


``` 



##Random Forest

```{r, results='markup'}
rfBS <- randomForest(stroke~., data=beforeSMOTE, proximity=TRUE) #beforeSMOTE
rf <- randomForest(stroke~., data=afterSMOTE, proximity=TRUE)  #afterSMOTE
```

```{r, results='markup'}
predict_testBS = predict(rfBS, test) #beforeSMOTE
confusionMatrix(predict_testBS, test$stroke)
```

```{r, results='markup'}
predict_test = predict(rf, test) #afterSMOTE
confusionMatrix(predict_test, test$stroke)
```

If we compare accuracy of the models (beforeSMOTE and afterSMOTE model), `accuracy` is higher for beforeSMOTE model. But if we check the confusion matrix, beforeSMOTE model is not giving any TRUE positive value. So, accuracy is not a very good indicator to assess the model.\
When medical datasets are involved, `precision` is a much better metric.\
Precision = TP/(TP+FP)\
So for `rfBS`, precision = 0\
and for `rf`, precision = `r 73/(73+1099)`\
So, we can see that afterSMOTE model is performing much better when it comes to predict the TRUE positives.\

```{r, results='markup'}
rfBS2 <- randomForest(stroke~age+hypertension+heart_disease+avg_glucose_level, data=beforeSMOTE, proximity=TRUE)  #model2 randomForest beforeSMOTE
rf2 <- randomForest(stroke~age+hypertension+heart_disease+avg_glucose_level, data=afterSMOTE, proximity=TRUE)  #model2 randomForest afterSMOTE
```

```{r, results='markup'}
predict_testBS2 = predict(rfBS2, test)  # beforeSMOTE
confusionMatrix(predict_testBS2, test$stroke)
```

```{r, results='markup'}
predict_test2 = predict(rf2, test)  #afterSMOTE
confusionMatrix(predict_test2, test$stroke)
```
The same thing is observed for model2`.\
Precision is much better for afterSMOTE dataset.\

```{r, results='markup'}
rfBS3 <- randomForest(stroke~age:hypertension+age:heart_disease+bmi, data=beforeSMOTE, proximity=TRUE) #beforeSMOTE
rf3 <- randomForest(stroke~age:hypertension+age:heart_disease+bmi, data=afterSMOTE, proximity=TRUE)
```

```{r, results='markup'}
predict_testBS3 = predict(rfBS3, test)  #beforeSMOTE
confusionMatrix(predict_testBS3, test$stroke)
```

```{r, results='markup'}
predict_test3 = predict(rf3, test)  #afterSMOTE
confusionMatrix(predict_test3, test$stroke)
```

Checking the precision of afterSMOTE model3\
Precision = `r 108/(108/1565)`\
Precision has improved if compared to model1 and model2.\

```{r}
# trControl <- trainControl(method = "cv",
#     number = 10,
#     search = "grid")
```

```{r}
# rf_cv <- train(stroke~.,    # we tried gridSearch CV to get the best accuracy, but it seems maximum accuracy is achieved by running it one time.
#     data = afterSMOTE,      # so commented this code as it takes a very long time to run the full code.
#     method = "rf",
#     metric = "Accuracy",
#     trControl = trControl)
# # Print the results
# print(rf_cv)

```


```{r}
# rf_cv1 <- train(stroke~age+hypertension+heart_disease+avg_glucose_level,
#     data = afterSMOTE,
#     method = "rf",
#     metric = "Accuracy",
#     trControl = trControl)
# # Print the results
# print(rf_cv1)

```

```{r}
# rf_cv2 <- train(stroke~age:hypertension+age:heart_disease+bmi,
#     data = afterSMOTE,
#     method = "rf",
#     metric = "Accuracy",
#     trControl = trControl)
# # Print the results
# print(rf_cv2)

```


```{r, results='markup'}
rfBS4 <- randomForest(stroke~bmi+avg_glucose_level, data=beforeSMOTE, proximity=TRUE)  #beforeSMOTE
rf4 <- randomForest(stroke~bmi+avg_glucose_level, data=afterSMOTE, proximity=TRUE)   #afterSMOTE
```

Since age is the most important factor for our random forest, so we tried removing age in the formula and keeping just the numeric variables.\

```{r, results='markup'}
predict_testBS4 = predict(rfBS4, test)   #beforeSMOTE
confusionMatrix(predict_testBS4, test$stroke)
```


```{r, results='markup'}
predict_test4 = predict(rf4, test)   #aftersMOTE
confusionMatrix(predict_test4, test$stroke)
```
As we can see from the results, precision dropped quite a bit. As it is only predicting 40 True positives.\
We can say that age is a very important predictor for our dataset.\

```{r}
rfBS5 <- randomForest(stroke~age, data=beforeSMOTE, proximity=TRUE)    #beforeSMOTE
```

```{r}
rf5 <- randomForest(stroke~age, data=afterSMOTE, proximity=TRUE)     #afterSMOTE
```

```{r}  
predict_testBS5 = predict(rfBS5, test)         #beforeSMOTE
confusionMatrix(predict_testBS5, test$stroke)
```

```{r}
predict_test5 = predict(rf5, test)    #afterSMOTE
confusionMatrix(predict_test5, test$stroke)
```


###Variable Importance Plots for beforeSMOTE dataset
```{r, results='markup'}
varImpPlot(rfBS)
varImpPlot(rfBS2)
varImpPlot(rfBS3)
```


###Variable Importance Plots for afterSMOTE dataset
```{r, results='markup'}
varImpPlot(rf)
varImpPlot(rf2)
varImpPlot(rf3)
```
From the variable importance plots,\
in beforeSMOTE models -> avg_glucose_level, bmi are more important as compared to age and the difference between the three is not very high.\
an afterSMOTE models -> age is a much more important predictor as compared to bmi and avg_glucose_level.\
The MeanDecreaseGini measures the Gini importance = how important the features are over all splits done in the tree/forest - whereas for each individual split the Gini importance indicates how much the Gini criterion = "inequality/heterogeneity" was reduced using this split.\


```{r, results='markup'}
partialPlot(rfBS3, afterSMOTE, age, "1")
partialPlot(rf3, afterSMOTE, age, "1")
```
We plotted a partial plot of dependence of age on getting a stroke.\
There are a few differences between the two plots, but overall they both are showing the same thing that higher age people tend to be more prone to stroke.\




###ROC curve beforeSMOTE dataset
```{r, results='markup'}
library(pROC)
rf_predictionBS <- predict(rfBS3, test, type = "prob")
ROC_rfBS <- roc(test$stroke, rf_predictionBS[,2])
ROC_rf_aucBS <- auc(ROC_rfBS)
plot(ROC_rfBS, col = "green", main = "ROC For Random Forest beforeSMOTE")

```


###ROC curve afterSMOTE dataset
```{r}
library(pROC)
rf_prediction <- predict(rf3, test, type = "prob")
ROC_rf <- roc(test$stroke, rf_prediction[,2])
ROC_rf_auc <- auc(ROC_rf)
plot(ROC_rf, col = "green", main = "ROC For Random Forest afterSMOTE")

```
According to the precision values, model3 is performing best in random forest.\
When comparing the ROC curve for beforeSMOTE and afterSMOTE models, AUC value is much higher for ROC afterSMOTE model.\
AUC for beforeSMOTE model: 0.65800\
AUC for afterSMOTE model: 0.77455\

Although none of them is crossing the threshhold of 0.8. but afterSMOTE models are coming out to be very close to the threshhold and much better than beforeSMOTE models.\


##Logistic Regression

### Logistic Regression on Original Dataset 
```{r}
#before smote training
logitModel1_b <- glm(stroke~ ., data = beforeSMOTE, family = binomial())
logitModel2_b <- glm(stroke~ age + age:hypertension + age:heart_disease +age:avg_glucose_level, data = beforeSMOTE, family = binomial())
logitModel3_b <- glm(stroke~  age*heart_disease +age*glucoseGroup, data = beforeSMOTE, family = binomial())
logitModel4_b <- glm(stroke~ (age+heart_disease+ avg_glucose_level)^2 , data = beforeSMOTE, family = binomial())
logitModel5_b <- glm(stroke~ (age + glucoseGroup)^2 , data = beforeSMOTE, family = binomial())
logitModel6_b <- glm(stroke~ age+age:heart_disease+ age:avg_glucose_level , data = beforeSMOTE, family = binomial())
#selecting model for further analysis of code
logitModel_b <-logitModel3_b
summary(logitModel_b)
```

```{r results='markup'}
pR2(logitModel_b)
AIC(logitModel_b)
```

### Logistic Regression on Upsampled Dataset
```{r results='markup'}
#after smote training
logitModel1_a <- glm(stroke~ ., data = afterSMOTE, family = binomial())
logitModel2_a <- glm(stroke~ age + age:hypertension + age:heart_disease +age:avg_glucose_level, data = afterSMOTE, family = binomial())
logitModel3_a <- glm(stroke~ age*heart_disease +age*glucoseGroup, data = afterSMOTE, family = binomial())
logitModel4_a <- glm(stroke~ (age+heart_disease+ avg_glucose_level)^2, data = afterSMOTE, family = binomial())
logitModel5_a <- glm(stroke~ (age + glucoseGroup)^2, data = afterSMOTE, family = binomial())
logitModel6_a <- glm(stroke~ age+age:heart_disease+ age:avg_glucose_level , data = afterSMOTE, family = binomial())
#selecting model for further analysis of code
logitModel_a <- logitModel3_a
summary(logitModel_a)
```

```{r results='markup'}
pR2(logitModel_a)
AIC(logitModel_a)
```

Here with the upsampled datset the psuedo R2 value has increased a lot which is because with more data points of 1, a lot more variance in y is explained by the variables now. 

### Training Testing Set Prediction Score Distribution

Checking the prediction density from the models for both training and testing dataset

####Before SMOTE
```{r results='markup'}

beforeSMOTE$prediction <- predict( logitModel_b, newdata = beforeSMOTE, type = "response" )
test$prediction  <- predict( logitModel_b, newdata = test , type = "response" )

par(mfrow = c(1, 2))
ggp1 <- ggplot( beforeSMOTE, aes( prediction, color = as.factor(stroke) ) ) + 
geom_density( size = 1 ) +
ggtitle( "Training Set's Predicted Score before SMOTE" ) 

ggp2 <- ggplot( test, aes( prediction, color = as.factor(stroke) ) ) + 
geom_density( size = 1 ) +
ggtitle( "Testing Set's Predicted Score before SMOTE" ) 
grid.arrange(ggp1, ggp2, ncol = 2) 
```

Now for before smote the cut-off value is very close to 0. Mostly between 0.0 to 0.05.\
But the reason this is not an accurate estimate is because of the difference in size of 0(~9000) and 1(~200) entries.\
even though the density is high for blue curve after the intersection point they are relative to the number of entries in each dataset. \

####After SMOTE

Here the optimal cut off can be the\ 

```{r results='markup'}
testAfterSMOTE <- test
afterSMOTE$prediction <- predict( logitModel_a, newdata = afterSMOTE, type = "response" )
testAfterSMOTE$prediction  <- predict( logitModel_a, newdata = test , type = "response" )
par(mfrow = c(1, 2))
ggp1 <- ggplot( afterSMOTE, aes( prediction, color = as.factor(stroke) ) ) + 
geom_density( size = 1 ) +
ggtitle( "Training Set's Predicted Score after SMOTE" ) 

ggp2 <- ggplot( testAfterSMOTE, aes( prediction, color = as.factor(stroke) ) ) + 
geom_density( size = 1 ) +
ggtitle( "Testing Set's Predicted Score after SMOTE" ) 
grid.arrange(ggp1, ggp2, ncol = 2) 
```

So as observed there is a slight shift in intersection point for cut off value. 
which is because after training on more data, it has learned better and started assigning greater probabilities to the 1 class. 

### Confusion Matrix plots at different accuracy

```{r}
ConfusionMatrixInfo <- function( data, predict, actual, cutoff )
{	
	predict <- data[[predict]]
	actual  <- relevel( as.factor( data[[actual]] ), "1" )
	
	result <- data.table( actual = actual, predict = predict )
	result[ , type := ifelse( predict >= cutoff & actual == 1, "TP",
					  ifelse( predict >= cutoff & actual == 0, "FP", 
					  ifelse( predict <  cutoff & actual == 1, "FN", "TN" ) ) ) %>% as.factor() ]

	# jittering : can spread the points along the x axis 
	plot <- ggplot( result, aes( actual, predict, color = type ) ) + 
			geom_violin( fill = "white", color = NA ) +
			geom_jitter( shape = 1 ) + 
			geom_hline( yintercept = cutoff, color = "blue", alpha = 0.6 ) + 
			scale_y_continuous( limits = c( 0, 1 ) ) + 
			scale_color_discrete( breaks = c( "TP", "FN", "FP", "TN" ) ) + # ordering of the legend 
			guides( col = guide_legend( nrow = 2 ) ) + # adjust the legend to have two rows  
			ggtitle( sprintf( "ConfusionMatrix Cutoff : %.2f", cutoff ) )

	return( list( data = result, plot = plot ))
}

```

####Left : before SMOTE, Right : after SMOTE
```{r results='markup'}

# visualize .6 cutoff (lowest point of the previous plot)
# cm_info <- ConfusionMatrixInfo( data = test, predict = "prediction", 
#                                 actual = "stroke", cutoff = .08 )
# #ggthemr("flat")
# cm_info$plot
par(mfrow = c(1, 2))
list1 <- ConfusionMatrixInfo( data = test, predict = "prediction", actual = "stroke", cutoff = .02 )
list2 <- ConfusionMatrixInfo( data = testAfterSMOTE, predict = "prediction", actual = "stroke", cutoff = .27 )
ggp1 <- list1$plot
ggp2 <- list2$plot
grid.arrange(ggp1, ggp2, ncol = 2)

```
The cutoff value that we selected are because of the below curves for(sensitivity, precision, f1 score). The plot of confusion matric is shown first to give a clear idea of the distribution.

### Accuracy versus Cutoff 
####Left : before SMOTE, Right : after SMOTE

```{r results='markup'}
# ------------------------------------------------------------------------------------------
# [AccuracyCutoffInfo] : 
# Obtain the accuracy on the trainining and testing dataset.
# for cutoff value ranging from .4 to .8 ( with a .05 increase )
# @train   : your data.table or data.frame type training data ( assumes you have the predicted score in it ).
# @test    : your data.table or data.frame type testing data
# @predict : prediction's column name (assumes the same for training and testing set)
# @actual  : actual results' column name
# returns  : 1. data : a data.table with three columns.
#            		   each row indicates the cutoff value and the accuracy for the 
#            		   train and test set respectively.
# 			 2. plot : plot that visualizes the data.table
AccuracyCutoffInfo <- function( train, test, predict, actual )
{ 
	cutoff <- seq( .01, .9, by = .05 )
	accuracy <- lapply( cutoff, function(c)
	{
	  data_train <- as.factor( as.numeric( train[[predict]] > c ) )
		cm_train <- confusionMatrix(data_train, as.factor(train[[actual]]),positive="1" )
		data_test <- as.factor( as.numeric( test[[predict]] > c ) )
		cm_test  <- confusionMatrix( data_test, as.factor(test[[actual]]),positive="1" )
			
		dt <- data.table( cutoff = c,
						  train  = cm_train$overall[["Accuracy"]],
		 			    test   = cm_test$overall[["Accuracy"]] )
		return(dt)
	}) %>% rbindlist()

	# visualize the accuracy of the train and test set for different cutoff value 
	# accuracy in percentage.
	accuracy_long <- gather( accuracy, "data", "accuracy", -1 )
	plot <- ggplot( accuracy_long, aes( cutoff, accuracy, group = data, color = data ) ) + 
			geom_line( size = 1 ) + geom_point( size = 3 ) +
			scale_y_continuous( label = percent ) +
			ggtitle( "Train/Test Accuracy vs Cutoff" )
	return( list( data = accuracy, plot = plot ) )
}
```


```{r results='markup'}
par(mfrow = c(1, 2))
list1 <- AccuracyCutoffInfo( train = beforeSMOTE, test = test, predict = "prediction", actual = "stroke" )
list2<- AccuracyCutoffInfo( train = afterSMOTE, test = testAfterSMOTE, predict = "prediction", actual = "stroke" )
ggp1 <- list1$plot
ggp2 <- list2$plot
grid.arrange(ggp1, ggp2, ncol = 2) 
```

Now if observe the accuracy curve,

* For before SMOTE, accuracy keeps on increasing an reaches a plateau after 0.1 \
reason : if the model classifies all data points as 0, it would achieve a accuracy of 9793/(9793+202) = 97.9% \

* For After SMOTE, since for training data we have a significant number of 1 , the probability values are distributed well between 0 to 1. and thus with the increase in cutoff. so the drop in accuracy of training dataset with the increase in cutoff is explained by the fact that a significant number of 1's are now wrongly predicted.

* Since testing data is similar, even with the well spreaded values of 0 and 1, even though the density of 1 is more above cutoff, the significantly high number of 0's are overpowering that thing and thus the accuracy keeps on increasing here as well for test dataset.

####So how should we evaluate our model then? 

The above results confirms that in cases with rare disease or imbalance, it is not a good idea to look at accuracy values. 
So instead of that we will focus more on the parameters that are more medically sound. \
\
What do you think can be more important for medical studies? \
We would be more interested in a model which is more sensitive toward people getting stroke.\ 

The parameters that would consider these things are :

* Sensitivity : Percentage of people with stroke who were correctly identified as getting stroke \ 

So that more number of people who are going to get stroke can be made caution and protected \

* Precision : the probability that a patient diagnosed as stroke by the classifier got stroke. \
So for whatever people the model is predicting as will get stroke, how many of them are actually getting it.
This will make the situation less chaotic among population and make the positive stroke value as more serious.\ 
\
Hence, we would be more interested in **Sensitivity and Precision** value.
Let's observe them for our model.

### Sensitivity versus cutoff 
```{r results='markup'}
SensitivityCutoffInfo <- function( train, test, predict, actual )
{
	cutoff <- seq( .01, .9, by = .05 )
	accuracy <- lapply( cutoff, function(c)
	{
	  data_train <- as.factor( as.numeric( train[[predict]] > c ) )
		cm_train <- confusionMatrix(data_train, as.factor(train[[actual]]),positive="1" )
		data_test <- as.factor( as.numeric( test[[predict]] > c ) )
		cm_test  <- confusionMatrix( data_test, as.factor(test[[actual]]),positive="1" )
			
		dt <- data.table( cutoff = c,
						  train  = cm_train$byClass[["Sensitivity"]],
		 			    test   = cm_test$byClass[["Sensitivity"]] )
		return(dt)
	}) %>% rbindlist()
	accuracy_long <- gather( accuracy, "data", "accuracy", -1 )
	
	plot <- ggplot( accuracy_long, aes( cutoff, accuracy, group = data, color = data ) ) + 
			geom_line( size = 1 ) + geom_point( size = 3 ) +
			scale_y_continuous( label = percent ) +
			ggtitle( "Train/Test Sensivity vs Cutoff" )+
	    ylab("Sensitivity")

	return( list( data = accuracy, plot = plot ) )
}
```


```{r results='markup'}
par(mfrow = c(1, 2))
list1 <- SensitivityCutoffInfo( train = beforeSMOTE, test = test, predict = "prediction", actual = "stroke" )
list2<- SensitivityCutoffInfo( train = afterSMOTE, test = testAfterSMOTE, predict = "prediction", actual = "stroke" )
ggp1 <- list1$plot
ggp2 <- list2$plot
grid.arrange(ggp1, ggp2, ncol = 2) 
```

So why is the sensitivity value decreasing so rapidly?\
Left curve - before SMOTE\
since all the predicted probabilities are between 0&0.25.
after 0 and 0.25, all the datapoints which had stroke = 1, are predicted as 0, the sensitivity is dropping so rapidly. \
\
Right Curve - after SMOTE\
train - since the number of 0 & 1 are more balanced, and the probability values are spreaded well between 0 - 1, the drop in sensitivity value is smooth.\
\
test - the datapoints are now well spreaded between 0 & 1, hence the curve is now smooth \

### Precision versus cutoff 
```{r results='markup'}
PrecisionCutoffInfo <- function( train, test, predict, actual )
{
	cutoff <- seq( .01, .9, by = .05 )
	accuracy <- lapply( cutoff, function(c)
	{
	  data_train <- as.factor( as.numeric( train[[predict]] > c ) )
		cm_train <- confusionMatrix(data_train, as.factor(train[[actual]]),positive="1" )
		data_test <- as.factor( as.numeric( test[[predict]] > c ) )
		cm_test  <- confusionMatrix( data_test, as.factor(test[[actual]]),positive="1" )
			
		dt <- data.table( cutoff = c,
						  train  = cm_train$byClass[["Precision"]],
		 			    test   = cm_test$byClass[["Precision"]] )
		return(dt)
	}) %>% rbindlist()
	accuracy_long <- gather( accuracy, "data", "accuracy", -1 )
	
	plot <- ggplot( accuracy_long, aes( cutoff, accuracy, group = data, color = data ) ) + 
			geom_line( size = 1 ) + geom_point( size = 3 ) +
			scale_y_continuous( label = percent ) +
			ggtitle( "Train/Test Precision vs Cutoff" )+
	    ylab("Precision")

	return( list( data = accuracy, plot = plot ) )
}
```


```{r results='markup'}
par(mfrow = c(1, 2))
list1 <- PrecisionCutoffInfo( train = beforeSMOTE, test = test, predict = "prediction", actual = "stroke" )
list2<- PrecisionCutoffInfo( train = afterSMOTE, test = testAfterSMOTE, predict = "prediction", actual = "stroke" )
ggp1 <- list1$plot
ggp2 <- list2$plot
grid.arrange(ggp1, ggp2, ncol = 2) 
```


Precision is an estimate of how reliable the 1(Stroke) value predicted by model is. \
so now since after the cut off of 0.25, no value is predicted as 1 by the model, FP = 0, and hence, the value is coming out as TP/TP 1.\

### F1 Score versus cutoff 

```{r results='markup'}
FOneCutoffInfo <- function( train, test, predict, actual )
{
	cutoff <- seq( .01, .9, by = .05 )
	accuracy <- lapply( cutoff, function(c)
	{
	  data_train <- as.factor( as.numeric( train[[predict]] > c ) )
		cm_train <- confusionMatrix(data_train, as.factor(train[[actual]]),positive="1" )
		data_test <- as.factor( as.numeric( test[[predict]] > c ) )
		cm_test  <- confusionMatrix( data_test, as.factor(test[[actual]]),positive="1" )
			
		dt <- data.table( cutoff = c,
						  train  = cm_train$byClass[["F1"]],
		 			    test   = cm_test$byClass[["F1"]] )
		return(dt)
	}) %>% rbindlist()
	accuracy_long <- gather( accuracy, "data", "accuracy", -1 )
	
	plot <- ggplot( accuracy_long, aes( cutoff, accuracy, group = data, color = data ) ) + 
			geom_line( size = 1 ) + geom_point( size = 3 ) +
			scale_y_continuous( label = percent ) +
			ggtitle( "Train/Test F1 vs Cutoff" )+
	    ylab("F1 Score")

	return( list( data = accuracy, plot = plot ) )
}
```


```{r results='markup'}
par(mfrow = c(1, 2))
list1 <- FOneCutoffInfo( train = beforeSMOTE, test = test, predict = "prediction", actual = "stroke" )
list2<- FOneCutoffInfo( train = afterSMOTE, test = testAfterSMOTE, predict = "prediction", actual = "stroke" )
ggp1 <- list1$plot
ggp2 <- list2$plot
grid.arrange(ggp1, ggp2, ncol = 2) 
```

The higher the precision recall score, the higher would be the F1 score, hence a high F1 score means a better model. \

### F1 Score versus cutoffConfusion Matrix at optimal cutoff 

Before SMOTE

```{r results='markup'}
c_b <- 0.02

beforeSMOTEpred <- as.factor( as.numeric( test[["prediction"]] > c_b ) )
cm_test_b <- confusionMatrix(beforeSMOTEpred, as.factor(test$stroke),positive="1" )
cm_test_b

```

* F1 score : `r cm_test_b$byClass[["F1"]]`
* Sensitivity  : `r cm_test_b$byClass[["Sensitivity"]]`
* Precision  : `r cm_test_b$byClass[["Precision"]]`

After SMOTE
```{r results='markup'}
c_a <- 0.27
afterSMOTEpred <- as.factor( as.numeric( testAfterSMOTE[["prediction"]] > c_a ) )
cm_test_a <- confusionMatrix(afterSMOTEpred, as.factor(testAfterSMOTE$stroke),positive="1" )
cm_test_a
```

* F1 score : `r cm_test_a$byClass[["F1"]]` 
* Sensitivity  : `r cm_test_a$byClass[["Sensitivity"]]`
* Precision  : `r cm_test_a$byClass[["Precision"]]`

### Deciding the cut off level 

This code will print the confusion matrix estimates at a range of cut off values  so that we have numbers for (accuracy,precision,sensitivity,f1score)
```{r results}
# setting the cut-off probablity
# function to print confusion matrices for diffrent cut-off levels of probability
#to get confusion matrix checking the max probability from the graph
#for before SMOTE
max_prob_b <- max(test$prediction)
min_prob_b <- min(test$prediction)
jump_b <-  0.01


# making sequence of cut-off probabilities       
cutoff_b <- seq( min_prob_b, max_prob_b, by = jump_b )
CmFn <- function(cutoff,model) {
       # predicting the test set results
         modelPred <- predict(model, test, type = "response")
         C1 <- ifelse(modelPred > cutoff, 1, 0)
         C2 <- test$stroke
         predY   <- as.factor(C1)
         actualY <- as.factor(C2)
        # use the confusionMatrix 
        cm1 <-confusionMatrix(table(predY,actualY),positive="1")
        #print(cutoff)
        #print(cm1$table)
        # extracting accuracy
        Accuracy <- cm1$overall[1]
        #print(Accuracy)
        # extracting sensitivity
          Sensitivity <- cm1$byClass[1]
        # extracting specificity
          Specificity <- cm1$byClass[2]
      # extracting value of Precision
          Precision <- cm1$byClass[5]
      #f1 score
          f1_score <- cm1$byClass[7] 
        # combined table
          tab <- cbind(cutoff,Accuracy,Sensitivity,Specificity,Precision,f1_score)
        return(tab)}

# loop using "lapply"
#tab2    <- lapply(cutoff1, CmFn)
tab_b    <- lapply(cutoff_b, CmFn,logitModel_b)
#tab2 <- CmFn(logitModel1_a,test,cutoff1)
#tab_b


       
```
```{r results='markup'}

max_prob_a <- max(testAfterSMOTE$prediction)
min_prob_a <- min(testAfterSMOTE$prediction)
jump_a <-  0.1

cutoff_a <- seq( min_prob_a, max_prob_a, by = jump_a )
tab_a <-lapply(cutoff_a, CmFn,logitModel_a)
```



### ROC - AUC Curve 

####for ubalanced dataset
```{r}
# loading the package

library(pROC)
#loadPkg("pROC") # receiver operating characteristic curve, gives the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The curve is on sensitivity/recall/true-positive-rate vs false_alarm/false-positive-rate/fall-out.
prob=predict(logitModel_b,test, type = "response")
#Admit$prob=prob
h <- roc(stroke~prob, data=test,levels = c(1, 0))
auc(h) # area-under-curve prefer 0.8 or higher.
plot(h,main="ROC-plot Before SMOTE")
```

Area under curve is 0.81\

####After SMOTEbalanced dataset 
```{r results='markup'}
prob=predict(logitModel_a,testAfterSMOTE, type = "response")
#Admit$prob=prob
h_logistic <- roc(stroke~prob, data=testAfterSMOTE)
auc(h_logistic) # area-under-curve prefer 0.8 or higher.
plot(h_logistic,main="ROC plot- After SMOTE")

```

Area under curve is 0.809\

Now why is there no significant difference in the ROC curve value ?\
This is because the testing dataset is same for both and the number of values for 1 are significantly small.\ 



### PR Curve
Always a good idea to plot a precision recall curve when we are more interested in one class out of the two.
Our aim is to make the curve as close to (1, 1) as possible- meaning a good precision and recall

```{r results='markup'}

# Predict
pred = predict(logitModel_b,test, type = "response")
# Store precision and recall scores at different cutoffs
par(mfrow = c(1, 2))
library(ROCR)
predobj <- prediction(pred, test$stroke)
perf <- performance(predobj,"prec", "rec")
ggp1<- plot(perf,main="PR Curve- BeforeSMOTE")

#Predict
pred = predict(logitModel_a,testAfterSMOTE, type = "response")
predobj <- prediction(pred, testAfterSMOTE$stroke)
perf <- performance(predobj,"prec", "rec")
ggp2<-plot(perf,main="PR Curve- After SMOTE")


grid.arrange(ggp1, ggp2, ncol = 2) 

```

###Solution

Our results will improve if we generate more entries in test dataset as well corresponding to stroke =1.\


```{r results='markup'}

testDataAfterSmote <- SMOTE(stroke ~ ., test, perc.over = 2000, perc.under = 100)
test_0 <- subset(testDataAfterSmote,testDataAfterSmote$stroke==0)
test_1 <- subset(testDataAfterSmote,testDataAfterSmote$stroke==1)
print(nrow(test_1))
print(nrow(test_0))

testDataAfterSmote$glucoseGroup=cut(testDataAfterSmote$avg_glucose_level, c(0, 114, 140, Inf), c("Normal", "Prediabetes", "Diabetes"), include.lowest=TRUE)
```


```{r results='markup'}

# Predict
pred = predict(logitModel_a,testDataAfterSmote, type = "response")
# Store precision and recall scores at different cutoffs
library(ROCR)
predobj <- prediction(pred, testDataAfterSmote$stroke)
perf <- performance(predobj,"prec", "rec")
ggp2<-plot(perf,main="PR curve -- test Data After SMOTE")


#prob=predict(logitModel_a,testAfterSMOTE, type = "response")
#Admit$prob=prob
h <- roc(stroke~pred, data=testDataAfterSmote)
auc(h) # area-under-curve prefer 0.8 or higher.

plot(h,main="ROC curve -- test Data After SMOTE")
```


Here we can observe the improvement in ROC, AUC=0.88 and PR curve.\
pr curve being close to (1,1) means a high sensitivity and high recall.
Since we have significant number of 0 and 1 values.

```{r results='markup'}
testDataAfterSmote$prediction = predict(logitModel_a,testDataAfterSmote, type = "response")
ggplot( testDataAfterSmote, aes( prediction, color = as.factor(stroke) ) ) + 
geom_density( size = 1 ) +
ggtitle( "Testing Set's Predicted Score after SMOTE" ) 

```

Conclusion : 

* we cannot upsample more datapoints in medical dataset so we will need to ask for more entries with the disease(stroke=1)
* or more significant terms for the model, which have a huge impact on the disease itself.


```{r, results='markup'}
plot(ROC_rf, col = "green", main = "ROC For all the three models")
plot(h_KNN, add=TRUE, col="red")
plot(h_logistic, add=TRUE, col='blue')
legend(0.3,0.4,legend=c("Random Forest", "Logistic Regression","KNN" ), 
       fill = c("green","blue", "red"))
```

