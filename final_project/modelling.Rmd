---
title: "Modelling"
author: "Team 6"
date: "2022-11-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Importing the required libraries
```{r}
library(caret)
library(dplyr)
library(lessR)
```


##Loading both the datasets 
```{r results='markup'}
beforeSMOTE <- data.frame(read.csv("beforeSmote.csv"))
afterSMOTE <- data.frame(read.csv("afterSmote.csv"))

```

##Factoring the Categorical variables as before 
```{r results='markup'}
#factoring the dataset
beforeSMOTE$gender = factor(beforeSMOTE$gender)
beforeSMOTE$hypertension = factor(beforeSMOTE$hypertension)
beforeSMOTE$heart_disease = factor(beforeSMOTE$heart_disease)
beforeSMOTE$ever_married = factor(beforeSMOTE$ever_married)
beforeSMOTE$work_type = factor(beforeSMOTE$work_type)
beforeSMOTE$Residence_type = factor(beforeSMOTE$Residence_type)
beforeSMOTE$smoking_status = factor(beforeSMOTE$smoking_status)
beforeSMOTE$stroke = factor(beforeSMOTE$stroke)
summary(beforeSMOTE)

#factoring the dataset
afterSMOTE$gender = factor(afterSMOTE$gender)
afterSMOTE$hypertension = factor(afterSMOTE$hypertension)
afterSMOTE$heart_disease = factor(afterSMOTE$heart_disease)
afterSMOTE$ever_married = factor(afterSMOTE$ever_married)
afterSMOTE$work_type = factor(afterSMOTE$work_type)
afterSMOTE$Residence_type = factor(afterSMOTE$Residence_type)
afterSMOTE$smoking_status = factor(afterSMOTE$smoking_status)
afterSMOTE$stroke = factor(afterSMOTE$stroke)
summary(afterSMOTE)

cols <-  hcl.colors(length(levels(beforeSMOTE$stroke)), "Fall")
PieChart(stroke, data = beforeSMOTE, hole = 0,
         fill = cols,
         labels_cex = 0.6)
```

### After SMOTE
```{r}
cols <-  hcl.colors(length(levels(afterSMOTE$stroke)), "Fall")
PieChart(stroke, data = afterSMOTE, hole = 0,
         fill = cols,
         labels_cex = 0.6)
```


## Setting the variable for checking 
```{r}
#set it to beforeSMOTE/afterSMOTE

modellingDataset <- beforeSMOTE
#modellingDataset <- afterSMOTE
```


## Splitting the dataset into train and test 
```{r}
set.seed(123)  
split1<- sample(c(rep(0, 0.7 * nrow(modellingDataset)), rep(1, 0.3 * nrow(modellingDataset))))
table(split1) 
#train split
train <- modellingDataset[split1 == 0, ]
#test split
test <- modellingDataset[split1== 1, ] 
```

###Logistic Regression
```{r results='markup'}
# fit logistic regression model 
logitModel <- glm(stroke~ ., 
                        data = train, 
                        family = binomial())

summary(logitModel)
```
Significant variables in case of beforeSMOTE 

* age 
* hypertension 
* heart_disease 
* avg_glucose_level 

Significant variables after applying SMOTE

* genderMale
* age
* hypertension1
* heart_disease1
* work_typeGovt_job
* smoking_statussmokes

#Predicting accuracy 
```{r }
logitModelPred <- predict(logitModel, test, type = "response")
# plot of probabilities
plot(logitModelPred, 
     main = "Scatterplot of Probabilities of stroke (test data)", 
     xlab = "Patients ", ylab = "Predicted Probability of stroke")
```

### Deciding the cut off level 
```{r results}
# setting the cut-off probablity
# function to print confusion matrices for diffrent cut-off levels of probability
#to get confusion matrix checking the max probability from the graph
max_prob <- 0.3
min_prob <- 0.001

CmFn <- function(cutoff) {

       # predicting the test set results
         logitModelPred <- predict(logitModel, test, type = "response")
         C1 <- ifelse(logitModelPred > cutoff, 1, 0)
         C2 <- test$stroke
         predY   <- as.factor(C1)
         actualY <- as.factor(C2)

        # use the confusionMatrix 
        cm1 <-confusionMatrix(table(predY,actualY))
        print(cutoff)
        print(cm1$table)
        # extracting accuracy
        Accuracy <- cm1$overall[1]
        print(Accuracy)
        # extracting sensitivity
          Sensitivity <- cm1$byClass[1]
        # extracting specificity
          Specificity <- cm1$byClass[2]
      # extracting value of kappa
          Kappa <- cm1$overall[2]

        # combined table
          tab <- cbind(cutoff,Accuracy,Sensitivity,Specificity,Kappa)
        return(tab)}
# making sequence of cut-off probabilities       
cutoff1 <- seq( min_prob, max_prob, by = .05 )
# loop using "lapply"
tab2    <- lapply(cutoff1, CmFn)
#tab2
       
```

#ROC Curve
```{r}
# loading the package
#library(ROCR)

library(pROC)
#loadPkg("pROC") # receiver operating characteristic curve, gives the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The curve is on sensitivity/recall/true-positive-rate vs false_alarm/false-positive-rate/fall-out.
prob=predict(logitModel,test, type = "response")
#Admit$prob=prob
h <- roc(stroke~prob, data=test)
auc(h) # area-under-curve prefer 0.8 or higher.
plot(h)
```

Standardization

```{r results='markup'}

```
```{r results='markup'}
train
```

```{r results='markup'}
numeric <- select(train, age, avg_glucose_level, bmi)
pc <- prcomp(numeric,
             center = TRUE,
            scale. = TRUE)
#numeric_test <- select(test, age, avg_glucose_level, bmi)
#pc_test <- prcomp(numeric_test,
#             center = TRUE,
#            scale. = TRUE)
#attributes(pc)
```

Converting  
```{r results='markup'}
#summary(pc)
pc_train <- predict(pc, train)
pc_test <- predict(pc,test)
pc_train <- data.frame(pc_train, train['stroke'])
pc_test <- data.frame(pc_test, test['stroke'])
```

```{r results='markup'}
pc_model <-  logitModel <- glm(stroke~ ., 
                        data = pc_train, 
                        family = binomial())
summary(pc_model)
```


```{r results='markup'}
prob=predict(pc_model,pc_test, type = "response")
h <- roc(stroke~prob, data=pc_test)
auc(h) # area-under-curve prefer 0.8 or higher.
plot(h)
```


#Predicting accuracy on PCA model
```{r }
logitModelPred <- predict(pc_model, pc_test, type = "response")
# plot of probabilities
plot(logitModelPred, 
     main = "Scatterplot of Probabilities of stroke (test data)", 
     xlab = "Patients", ylab = "Predicted Probability of stroke")
```

### Deciding the cut off level for PCA
```{r results}
# setting the cut-off probablity
# function to print confusion matrices for different cut-off levels of probability
#to get confusion matrix checking the max probability from the graph
max_prob <- 0.15
min_prob <- 0.001

CmFn <- function(cutoff) {

       # predicting the test set results
         logitModelPred <- predict(pc_model, pc_test, type = "response")
         C1 <- ifelse(logitModelPred > cutoff, 1, 0)
         C2 <- pc_test$stroke
         predY   <- as.factor(C1)
         actualY <- as.factor(C2)
         print(head(predY))
         print(head(actualY))
        # use the confusionMatrix 
        cm1 <-confusionMatrix(table(predY,actualY))
        print(cutoff)
        print(cm1$table)
        # extracting accuracy
        Accuracy <- cm1$overall[1]
        print(Accuracy)
        # extracting sensitivity
          Sensitivity <- cm1$byClass[1]
        # extracting specificity
          Specificity <- cm1$byClass[2]
      # extracting value of kappa
          Kappa <- cm1$overall[2]

        # combined table
          tab <- cbind(cutoff,Accuracy,Sensitivity,Specificity,Kappa)
        return(tab)}
# making sequence of cut-off probabilities       
cutoff1 <- seq( min_prob, max_prob, by = .05 )
# loop using "lapply"
tab2    <- lapply(cutoff1, CmFn)
tab2
       
```
